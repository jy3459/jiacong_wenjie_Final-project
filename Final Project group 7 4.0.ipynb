{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "Done by Group7 : Jiacong Yuan ( jy3459 ) and Wenjie Lin ( wl2792 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from functools import partial\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones and Lookup Coordinates\n",
    "This section demonstrates how to load a geospatial dataset representing NYC taxi zones and find the geographic coordinates (latitude and longitude) for a specific taxi zone using its unique Location ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882de2ce-006b-416f-89fd-09662c05b5ec",
   "metadata": {},
   "source": [
    "### Step 1: Load the Taxi Zones Data\n",
    "\n",
    "#### **What Happens in This Step**\n",
    "\n",
    "1. **Input File Path**  \n",
    "   - The file path for the shapefile is specified. It should contain a map of NYC taxi zones, each associated with a unique `LocationID`.\n",
    "\n",
    "2. **Check for File Existence**  \n",
    "   - The program first verifies if the shapefile exists at the specified path. If the file is missing, it raises an error and stops the execution.\n",
    "\n",
    "3. **Load the Shapefile**  \n",
    "   - The file is loaded into a GeoDataFrame using GeoPandas, which is specialized for working with spatial data.\n",
    "\n",
    "4. **Validate the Data**  \n",
    "   - It ensures that the shapefile contains the required column `LocationID` (a unique identifier for each taxi zone).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5772e96f-ccab-48fd-ae94-0ca34e24889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile_path):\n",
    "    \"\"\"\n",
    "    Load the Taxi Zone shapefile into a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        shapefile_path (str): Path to the Taxi Zone shapefile.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: A GeoDataFrame containing Taxi Zone data.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the shapefile does not exist.\n",
    "        RuntimeError: If there is an error loading the shapefile.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(shapefile_path):\n",
    "        raise FileNotFoundError(f\"Shapefile not found at: {shapefile_path}\")\n",
    "\n",
    "    try:\n",
    "        taxi_zones_gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        if 'LocationID' not in taxi_zones_gdf.columns:\n",
    "            raise ValueError(\"Shapefile must contain 'LocationID' column.\")\n",
    "\n",
    "        # Ensure CRS is EPSG:4326\n",
    "        if taxi_zones_gdf.crs is None or taxi_zones_gdf.crs.to_epsg() != 4326:\n",
    "            taxi_zones_gdf = taxi_zones_gdf.to_crs(epsg=4326)\n",
    "\n",
    "        return taxi_zones_gdf\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load shapefile: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi Zone Shapefile found at: C:\\Users\\wenji\\OneDrive\\Project\\taxi_zones/taxi_zones.shp\n"
     ]
    }
   ],
   "source": [
    "TAXI_ZONES_DIR = r\"C:\\Users\\wenji\\OneDrive\\Project\\taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "if not os.path.exists(TAXI_ZONES_SHAPEFILE):\n",
    "    raise FileNotFoundError(f\"Taxi Zone Shapefile not found at: {TAXI_ZONES_SHAPEFILE}\")\n",
    "else:\n",
    "    print(f\"Taxi Zone Shapefile found at: {TAXI_ZONES_SHAPEFILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ffaea61-39d0-4a72-9f01-da0663b8413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
      "0         1    0.116357    0.000782           Newark Airport           1   \n",
      "1         2    0.433470    0.004866              Jamaica Bay           2   \n",
      "2         3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
      "3         4    0.043567    0.000112            Alphabet City           4   \n",
      "4         5    0.092146    0.000498            Arden Heights           5   \n",
      "\n",
      "         borough                                           geometry  \n",
      "0            EWR  POLYGON ((-74.18445 40.695, -74.18449 40.6951,...  \n",
      "1         Queens  MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ...  \n",
      "2          Bronx  POLYGON ((-73.84793 40.87134, -73.84725 40.870...  \n",
      "3      Manhattan  POLYGON ((-73.97177 40.72582, -73.97179 40.725...  \n",
      "4  Staten Island  POLYGON ((-74.17422 40.56257, -74.17349 40.562...  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    taxi_zones_gdf = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "    print(taxi_zones_gdf.head())\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading shapefile: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e2f4e4-de25-4f22-8a8b-377d0fad9422",
   "metadata": {},
   "source": [
    "### Step 2: Lookup Coordinates for a Taxi Zone\n",
    "\n",
    "#### **What Happens in This Step**\n",
    "\n",
    "1. **Input Location ID**  \n",
    "   - The user provides a `LocationID`, a unique identifier for a specific taxi zone.\n",
    "\n",
    "2. **Filter the Data**  \n",
    "   - The program searches the GeoDataFrame to find the zone with the matching `LocationID`.\n",
    "\n",
    "3. **Extract the Geometry**  \n",
    "   - Once the zone is identified, its shape (geometry) is used to calculate the centroid (center point).\n",
    "\n",
    "4. **Output Coordinates**  \n",
    "   - The program returns the coordinates (latitude and longitude) of the centroid.  \n",
    "   - If the `LocationID` is not found in the data, it returns `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f3dabf-56d0-4d11-9b2f-0bc20e555b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    \"\"\"\n",
    "    Lookup the latitude and longitude for a given Taxi Zone Location ID.\n",
    "\n",
    "    Parameters:\n",
    "        zone_loc_id (int): Taxi Zone Location ID.\n",
    "        loaded_taxi_zones (gpd.GeoDataFrame): GeoDataFrame containing Taxi Zone data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (latitude, longitude) of the centroid of the zone geometry, \n",
    "               or None if the Location ID is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter GeoDataFrame for the given Location ID\n",
    "        zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "        \n",
    "        if zone.empty:\n",
    "            return None  # Return None if the Location ID is not found\n",
    "        \n",
    "        # Extract the geometry and calculate the centroid\n",
    "        centroid = zone.iloc[0].geometry.centroid\n",
    "        \n",
    "        # Return the centroid's coordinates as (latitude, longitude)\n",
    "        return centroid.y, centroid.x  # (latitude, longitude)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error looking up coordinates for Zone ID {zone_loc_id}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "408034ed-005e-4936-a93a-642db7c3f6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates for Zone ID 263: Latitude = 40.77876585543437, Longitude = -73.951009874818\n"
     ]
    }
   ],
   "source": [
    "# Lookup coordinates for a specific Taxi Zone ID\n",
    "zone_id = 263\n",
    "coords = lookup_coords_for_taxi_zone_id(zone_id, taxi_zones_gdf)\n",
    "\n",
    "if coords:\n",
    "    print(f\"Coordinates for Zone ID {zone_id}: Latitude = {coords[0]}, Longitude = {coords[1]}\")\n",
    "else:\n",
    "    print(f\"Zone ID {zone_id} not found in the Taxi Zone data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size\n",
    "This section demonstrates how to calculate the appropriate sample size for analyzing a dataset using Cochran's formula, specifically tailored for finite populations like taxi trip data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60091e1f-895d-4ee4-bbe9-eb086abc53d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To calculate the required sample size, we use **Cochran's formula**, which accounts for the population size and desired confidence level, margin of error, and proportion.\n",
    "\n",
    "1. **Input Parameters**  \n",
    "   - Population size: Total number of records in the dataset.  \n",
    "   - Confidence level: Typically 90%, 95%, or 99% (e.g., 95% confidence).  \n",
    "   - Margin of error: The acceptable error range for the results (e.g., ±5%).  \n",
    "   - Proportion: Estimated proportion of the population with the desired attribute (default is 0.5 for maximum variability).\n",
    "\n",
    "2. **Cochran's Formula for Infinite Populations**  \n",
    "   - The formula calculates the sample size assuming an infinite population:\n",
    "     $$\n",
    "     n_0 = \\frac{Z^2 \\cdot p \\cdot (1 - p)}{E^2}\n",
    "     $$\n",
    "     Where:\n",
    "     - \\( n_0 \\): Initial sample size for infinite population.\n",
    "     - \\( Z \\): Z-score based on the confidence level (e.g., 1.96 for 95% confidence).\n",
    "     - \\( p \\): Proportion of the population (default 0.5).  \n",
    "     - \\( E \\): Margin of error (e.g., 0.05 for ±5%).\n",
    "\n",
    "3. **Adjust for Finite Population Size**  \n",
    "   - If the population is finite, the sample size is adjusted using the correction formula:\n",
    "     $$\n",
    "     n = \\frac{n_0}{1 + \\frac{n_0 - 1}{N}}\n",
    "     $$\n",
    "     Where:\n",
    "     - \\( n \\): Final sample size for the finite population.  \n",
    "     - \\( N \\): Total population size.\n",
    "\n",
    "4. **Output**  \n",
    "   - The final sample size is rounded up to ensure a whole number of samples is selected.  \n",
    "   - This ensures the sample is statistically valid for the specified parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fb62f59-753b-45bb-bc0d-9d0c54e93ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population, confidence_level=0.95, margin_of_error=0.05, proportion=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the sample size using Cochran's formula for finite populations.\n",
    "\n",
    "    Parameters:\n",
    "        population (int): The size of the population (e.g., number of trips in a month).\n",
    "        confidence_level (float): The confidence level (default is 0.95 for 95% confidence).\n",
    "        margin_of_error (float): The margin of error (default is 0.05 for ±5% error).\n",
    "        proportion (float): The estimated proportion of the population with the desired attribute (default is 0.5).\n",
    "\n",
    "    Returns:\n",
    "        int: The calculated sample size.\n",
    "    \"\"\"\n",
    "    # Z-score for the given confidence level\n",
    "    z_scores = {0.9: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "    if confidence_level not in z_scores:\n",
    "        raise ValueError(\"Unsupported confidence level. Use 0.9, 0.95, or 0.99.\")\n",
    "    Z = z_scores[confidence_level]\n",
    "    \n",
    "    # Cochran's formula for an infinite population\n",
    "    n_0 = (Z**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust for finite population size\n",
    "    if population < n_0:\n",
    "        sample_size = population  # If the population is small, the entire dataset is needed\n",
    "    else:\n",
    "        sample_size = n_0 / (1 + (n_0 - 1) / population)\n",
    "    \n",
    "    return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population size: 6405008\n"
     ]
    }
   ],
   "source": [
    "yellow_taxi_df = pd.read_parquet(r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-01.parquet\")\n",
    "\n",
    "population_size = len(yellow_taxi_df)\n",
    "print(f\"Population size: {population_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db97f4-151b-4d90-8fe4-689157219647",
   "metadata": {},
   "source": [
    "### Step 1: Define a function `get_all_urls_from_tlc_page` that extracts all URLs from a specified webpage.\n",
    "\n",
    "1. **Input**:  \n",
    "   - The function takes a webpage URL (`taxi_page`) as input.\n",
    "\n",
    "2. **Request the Webpage**:  \n",
    "   - Using the `requests` library, the HTML content of the page is fetched.\n",
    "\n",
    "3. **Parse the HTML**:  \n",
    "   - The HTML content is parsed using `BeautifulSoup` from the `bs4` library.\n",
    "\n",
    "4. **Extract URLs**:  \n",
    "   - All anchor (`<a>`) tags with `href` attributes are located, and their links are extracted into a list.\n",
    "\n",
    "5. **Output**:  \n",
    "   - A list of extracted URLs is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_tlc_page(taxi_page):\n",
    "    \"\"\"\n",
    "    Extract all URLs from the TLC trip record data page.\n",
    "\n",
    "    Parameters:\n",
    "        taxi_page (str): URL of the TLC webpage.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of all extracted URLs from the webpage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Request the HTML page\n",
    "        response = requests.get(taxi_page)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all links on the page\n",
    "        links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error fetching or parsing the TLC page: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aabb8faf-c83d-489a-8bb9-91193037876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 563 URLs:\n",
      "http://www1.nyc.gov\n",
      "/311/index.page\n",
      "/home/search/index.page\n",
      "#\n",
      "/site/tlc/index.page\n",
      "http://www1.nyc.gov/home/text-size.page\n",
      "#\n",
      "/site/tlc/index.page\n",
      "/site/tlc/about/about-tlc.page\n",
      "/site/tlc/passengers/your-ride.page\n",
      "/site/tlc/drivers/get-a-drivers-license.page\n",
      "/site/tlc/vehicles/get-a-vehicle-license.page\n",
      "/site/tlc/businesses/yellow-cab.page\n",
      "/site/tlc/tlc-online/tlc-online.page\n",
      "/site/tlc/about/about-tlc.page\n",
      "/site/tlc/about/data-and-research.page\n",
      "/site/tlc/about/tlc-initiatives.page\n",
      "/site/tlc/about/contact-tlc.page\n",
      "/site/tlc/about/data.page\n",
      "/site/tlc/about/pilot-programs.page\n",
      "/site/tlc/about/industry-reports.page\n",
      "/site/tlc/about/tlc-trip-record-data.page\n",
      "/site/tlc/about/request-data.page\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "javascript:expandAll();\n",
      "javascript:collapseAll();\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-02.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-08.parquet\n",
      " https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-12.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-12.parquet\n",
      "/assets/tlc/downloads/pdf/trip_record_user_guide.pdf\n",
      "/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\n",
      "/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf\n",
      "/assets/tlc/downloads/pdf/data_dictionary_trip_records_fhv.pdf\n",
      "/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf\n",
      "/assets/tlc/downloads/pdf/working_parquet_format.pdf\n",
      "https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip\n",
      "/assets/tlc/images/content/pages/about/taxi_zone_map_bronx.jpg\n",
      "/assets/tlc/images/content/pages/about/taxi_zone_map_brooklyn.jpg\n",
      "/assets/tlc/images/content/pages/about/taxi_zone_map_manhattan.jpg\n",
      "/assets/tlc/images/content/pages/about/taxi_zone_map_queens.jpg\n",
      "/assets/tlc/images/content/pages/about/taxi_zone_map_staten_island.jpg\n",
      "/nyc-resources/agencies.page\n",
      "/home/contact-us.page\n",
      "https://a127-ess.nyc.gov\n",
      "https://a858-nycnotify.nyc.gov/notifynyc/\n",
      "https://a856-citystore.nyc.gov/\n",
      "/connect/social-media.page\n",
      "/connect/mobile-applications.page\n",
      "/nyc-resources/nyc-maps.page\n",
      "/nyc-resources/resident-toolkit.page\n",
      "/home/privacy-policy.page\n",
      "/home/terms-of-use.page\n",
      "https://www.nyc.gov/nyc-resources/website-accessibility-statement.page\n"
     ]
    }
   ],
   "source": [
    "# TLC Trip Record Data Page URL\n",
    "URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Extract all URLs\n",
    "try:\n",
    "    all_urls = get_all_urls_from_tlc_page(URL)\n",
    "    print(f\"Extracted {len(all_urls)} URLs:\")\n",
    "    for url in all_urls:  # Print all links in  URL\n",
    "        print(url)\n",
    "except RuntimeError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71663e7b-38d8-4636-8a39-59e88a5d1a05",
   "metadata": {},
   "source": [
    "### Step 2: The `filter_parquet_urls` function uses regular expressions to match and filter URLs for Yellow Taxi and HVFHV datasets. \n",
    "\n",
    "1. **Input**:  \n",
    "   - A list of URLs (`all_urls`) extracted from the TLC page.\n",
    "\n",
    "2. **Regular Expressions**:  \n",
    "   - Patterns are defined to match Parquet files for:\n",
    "     - Yellow Taxi datasets (`yellow_tripdata`).\n",
    "     - HVFHV datasets (`fhvhv_tripdata`).\n",
    "   - The date ranges included are:\n",
    "     - 2020 (January to December).\n",
    "     - 2021–2023 (all months).\n",
    "     - 2024 (January to August).\n",
    "\n",
    "3. **Filter URLs**:  \n",
    "   - URLs matching the patterns are extracted into separate lists.\n",
    "\n",
    "4. **Output**:  \n",
    "   - Two lists: one for Yellow Taxi dataset links, and another for HVFHV dataset links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(all_urls):\n",
    "    \"\"\"\n",
    "    Filter Yellow Taxi and HVFHV Parquet file URLs for the specified date range.\n",
    "\n",
    "    Parameters:\n",
    "        all_urls (list): A list of all URLs extracted from the TLC page.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two lists containing filtered URLs for Yellow Taxi and HVFHV datasets.\n",
    "    \"\"\"\n",
    "    # Compile regular expressions to match the desired Parquet files\n",
    "    yellow_taxi_pattern = re.compile(\n",
    "        r'yellow_tripdata_2020-(0[1-9]|1[0-2])\\.parquet|'\n",
    "        r'yellow_tripdata_202[1-3]-\\d{2}\\.parquet|'\n",
    "        r'yellow_tripdata_2024-(0[1-8])\\.parquet'\n",
    "    )\n",
    "    fhvhv_pattern = re.compile(\n",
    "        r'fhvhv_tripdata_2020-(0[1-9]|1[0-2])\\.parquet|'\n",
    "        r'fhvhv_tripdata_202[1-3]-\\d{2}\\.parquet|'\n",
    "        r'fhvhv_tripdata_2024-(0[1-8])\\.parquet'\n",
    "    )\n",
    "\n",
    "    # Filter URLs using the patterns\n",
    "    yellow_taxi_links = [url for url in all_urls if yellow_taxi_pattern.search(url)]\n",
    "    fhvhv_links = [url for url in all_urls if fhvhv_pattern.search(url)]\n",
    "\n",
    "    return yellow_taxi_links, fhvhv_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d360f474-3f1c-448e-87e0-9bd9c1c948d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Taxi Links:\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet\n",
      "\n",
      "FHVHV Links:\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-12.parquet\n"
     ]
    }
   ],
   "source": [
    "# Example: Extract URLs from the TLC page\n",
    "all_urls = get_all_urls_from_tlc_page(\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\")\n",
    "\n",
    "# Filter the Parquet file URLs\n",
    "yellow_taxi_links, fhvhv_links = filter_parquet_urls(all_urls)\n",
    "\n",
    "# Print the filtered links\n",
    "print(\"Yellow Taxi Links:\")\n",
    "for link in yellow_taxi_links:\n",
    "    print(link)\n",
    "\n",
    "print(\"\\nFHVHV Links:\")\n",
    "for link in fhvhv_links:\n",
    "    print(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf2ded-aa8d-4f60-9229-2cae7020b02b",
   "metadata": {},
   "source": [
    "### Step 3: Define the Download Function\n",
    "\n",
    "The `download_files` function takes a list of file URLs and downloads them to a specified directory.\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `file_links`: A list of URLs for the files to download.\n",
    "   - `save_dir`: Path to the directory where files will be saved.\n",
    "\n",
    "2. **Ensure Save Directory Exists**:\n",
    "   - The function creates the directory if it does not already exist using `os.makedirs`.\n",
    "\n",
    "3. **Download Files**:\n",
    "   - For each URL in `file_links`:\n",
    "     - The file name is extracted from the URL.\n",
    "     - The file is downloaded using `requests` and saved locally in the specified directory.\n",
    "     - The function prints the status of each download.\n",
    "\n",
    "4. **Handle Errors**:\n",
    "   - If any error occurs during the download, an appropriate error message is printed.\n",
    "\n",
    "5. **Use Download Function**:\n",
    "   - we use download function to download parquet file from yellow_taxi_links, fhvhv_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "419da674-f577-4e95-a8b7-5d170be9ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(file_links, save_dir):\n",
    "    \"\"\"\n",
    "    Downloads files from a list of URLs and saves them to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        file_links (list): List of URLs for the files to download.\n",
    "        save_dir (str): Path to the directory where files will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for link in file_links:\n",
    "        try:\n",
    "            # Extract file name from the URL\n",
    "            file_name = link.split(\"/\")[-1]\n",
    "            file_path = os.path.join(save_dir, file_name)\n",
    "            \n",
    "            # Download the file\n",
    "            print(f\"Downloading {file_name}...\")\n",
    "            response = requests.get(link, stream=True)\n",
    "            response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "            # Save the file locally\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(f\"Downloaded: {file_name}\")\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download {link}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41021bb6-eeff-4a61-b1c0-b1ebd817f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "file_links = [yellow_taxi_links]\n",
    "\n",
    "save_dir = \"C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\"\n",
    "download_files(file_links, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e2acd-7ba0-4234-b83c-501d37c0baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_links = fhvhv_links  # Use the list directly, assuming fhvhv_links is already a list of URLs\n",
    "save_dir = \"C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV\"\n",
    "\n",
    "download_files(file_links, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: Define the Sampling Function for a Single Parquet File\n",
    "\n",
    "The `sample_parquet_file` function loads a Parquet file, calculates the required sample size using Cochran's formula, extracts a random sample of rows, and saves the sampled data to a new Parquet file.\n",
    "\n",
    "1. **Process**:\n",
    "   - Loads the dataset from the specified Parquet file.\n",
    "   - Calculates the sample size using the `calculate_sample_size` function.\n",
    "   - Extracts a random sample of rows if the sample size is less than the population size. Otherwise, uses the full dataset.\n",
    "   - Saves the sampled data as a new Parquet file in the output directory.\n",
    "\n",
    "2. **Output**:\n",
    "   - A new Parquet file containing the sampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_parquet_file(parquet_file, output_directory=\"sampled yellow taxi\", confidence_level=0.95, margin_of_error=0.05, proportion=0.5):\n",
    "    \"\"\"\n",
    "    Load a Parquet file, calculate a sample size, extract a random sample,\n",
    "    and save it to a new Parquet file in the 'sampled yellow taxi' folder.\n",
    "\n",
    "    Parameters:\n",
    "        parquet_file (str): Path to the input Parquet file.\n",
    "        output_directory (str): Directory to save the sampled Parquet file.\n",
    "        confidence_level (float): Confidence level for sample size calculation (default is 0.95).\n",
    "        margin_of_error (float): Margin of error for sample size calculation (default is 0.05).\n",
    "        proportion (float): Estimated proportion of the population with the desired attribute (default is 0.5).\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Load the dataset\n",
    "    print(f\"Loading Parquet file: {parquet_file}\")\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "\n",
    "    # Calculate sample size\n",
    "    population_size = len(df)\n",
    "    sample_size = calculate_sample_size(population_size, confidence_level, margin_of_error, proportion)\n",
    "    print(f\"Population size: {population_size}, Sample size: {sample_size}\")\n",
    "\n",
    "    # If the sample size is larger than the population, use the full dataset\n",
    "    if sample_size >= population_size:\n",
    "        print(\"Sample size is larger than or equal to the population. Using the entire dataset.\")\n",
    "        sampled_df = df\n",
    "    else:\n",
    "        # Extract random sample\n",
    "        print(f\"Sampling {sample_size} rows from the dataset...\")\n",
    "        sampled_df = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    # Save the sampled dataset\n",
    "    file_name = os.path.basename(parquet_file)\n",
    "    output_file = os.path.join(output_directory, os.path.splitext(file_name)[0] + \"_sample.parquet\")\n",
    "    print(f\"Saving sampled data to: {output_file}\")\n",
    "    sampled_df.to_parquet(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e13f9-9f7e-4490-9e40-ea9c708ad2ce",
   "metadata": {},
   "source": [
    "### Step 5: Apply Sampling to All Files in a Directory\n",
    "\n",
    "The `sample_all_parquet_files` function applies the `sample_parquet_file` function to all Parquet files in a specified directory.\n",
    "\n",
    "1. **Process**:\n",
    "   - Ensures the `output_directory` exists or creates it.\n",
    "   - Iterates through all files in the `input_directory`.\n",
    "   - Identifies Parquet files by their `.parquet` extension.\n",
    "   - Applies the `sample_parquet_file` function to each Parquet file.\n",
    "\n",
    "2. **Output**:\n",
    "   - Sampled datasets are saved as new Parquet files in the `output_directory`, with filenames suffixed by `_sample.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_all_parquet_files(input_directory, output_directory=\"sampled yellow taxi\", confidence_level=0.95, margin_of_error=0.05, proportion=0.5):\n",
    "    \"\"\"\n",
    "    Apply sample_parquet_file to all Parquet files in a directory.\n",
    "\n",
    "    Parameters:\n",
    "        input_directory (str): Directory containing the Parquet files.\n",
    "        output_directory (str): Directory to save the sampled Parquet files.\n",
    "        confidence_level (float): Confidence level for sample size calculation (default is 0.95).\n",
    "        margin_of_error (float): Margin of error for sample size calculation (default is 0.05).\n",
    "        proportion (float): Estimated proportion of the population with the desired attribute (default is 0.5).\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Iterate through all files in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".parquet\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            # Apply sampling to each file\n",
    "            try:\n",
    "                sample_parquet_file(file_path, output_directory, confidence_level, margin_of_error, proportion)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process file {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "200776ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-01.parquet\n",
      "Population size: 6405008, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-02.parquet\n",
      "Population size: 6299367, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-03.parquet\n",
      "Population size: 3007687, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-04.parquet\n",
      "Population size: 238073, Sample size: 384\n",
      "Sampling 384 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-05.parquet\n",
      "Population size: 348415, Sample size: 384\n",
      "Sampling 384 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-06.parquet\n",
      "Population size: 549797, Sample size: 384\n",
      "Sampling 384 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-07.parquet\n",
      "Population size: 800412, Sample size: 384\n",
      "Sampling 384 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-08.parquet\n",
      "Population size: 1007286, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-09.parquet\n",
      "Population size: 1341017, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-10.parquet\n",
      "Population size: 1681132, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-11.parquet\n",
      "Population size: 1509000, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-12.parquet\n",
      "Population size: 1461898, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-12_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-01.parquet\n",
      "Population size: 1369769, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-02.parquet\n",
      "Population size: 1371709, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-03.parquet\n",
      "Population size: 1925152, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-04.parquet\n",
      "Population size: 2171187, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-05.parquet\n",
      "Population size: 2507109, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-06.parquet\n",
      "Population size: 2834264, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-07.parquet\n",
      "Population size: 2821746, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-08.parquet\n",
      "Population size: 2788757, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-09.parquet\n",
      "Population size: 2963793, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-10.parquet\n",
      "Population size: 3463504, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-11.parquet\n",
      "Population size: 3472949, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-12.parquet\n",
      "Population size: 3214369, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-12_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-01.parquet\n",
      "Population size: 2463931, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-02.parquet\n",
      "Population size: 2979431, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-03.parquet\n",
      "Population size: 3627882, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-04.parquet\n",
      "Population size: 3599920, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-05.parquet\n",
      "Population size: 3588295, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-06.parquet\n",
      "Population size: 3558124, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-07.parquet\n",
      "Population size: 3174394, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-08.parquet\n",
      "Population size: 3152677, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-09.parquet\n",
      "Population size: 3183767, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-10.parquet\n",
      "Population size: 3675411, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-11.parquet\n",
      "Population size: 3252717, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-12.parquet\n",
      "Population size: 3399549, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-12_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-01.parquet\n",
      "Population size: 3066766, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-02.parquet\n",
      "Population size: 2913955, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-03.parquet\n",
      "Population size: 3403766, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-04.parquet\n",
      "Population size: 3288250, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-05.parquet\n",
      "Population size: 3513649, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-06.parquet\n",
      "Population size: 3307234, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-07.parquet\n",
      "Population size: 2907108, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-08.parquet\n",
      "Population size: 2824209, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-09.parquet\n",
      "Population size: 2846722, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-10.parquet\n",
      "Population size: 3522285, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-11.parquet\n",
      "Population size: 3339715, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-12.parquet\n",
      "Population size: 3376567, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-12_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-01.parquet\n",
      "Population size: 2964624, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-02.parquet\n",
      "Population size: 3007526, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-03.parquet\n",
      "Population size: 3582628, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-04.parquet\n",
      "Population size: 3514289, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-05.parquet\n",
      "Population size: 3723833, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-06.parquet\n",
      "Population size: 3539193, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-07.parquet\n",
      "Population size: 3076903, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-08.parquet\n",
      "Population size: 2979183, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-08_sample.parquet\n"
     ]
    }
   ],
   "source": [
    "input_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\"\n",
    "output_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\"\n",
    "\n",
    "# Apply sampling to all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104b4da-32c3-4f9e-a805-24b4d0e2cbc3",
   "metadata": {},
   "source": [
    "### Process Taxi data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3e7ea-6ba4-4d58-8d51-aa37d6250e98",
   "metadata": {},
   "source": [
    "### Step 1: Define the Cleaning Function\n",
    "\n",
    "The `clean_parquet_columns` function reads a Parquet file, selects specific columns, and returns the cleaned DataFrame.\n",
    "\n",
    "1. **Input**:\n",
    "   - `parquet_file`: Path to the Parquet file that needs cleaning.\n",
    "\n",
    "2. **Process**:\n",
    "   - Reads the Parquet file into a Pandas DataFrame.\n",
    "   - Specifies the relevant columns to keep:\n",
    "     - `'tpep_pickup_datetime'`: Pickup time.\n",
    "     - `'tpep_dropoff_datetime'`: Dropoff time.\n",
    "     - `'passenger_count'`: Number of passengers.\n",
    "     - `'trip_distance'`: Distance of the trip.\n",
    "     - `'PULocationID'`: Pickup location ID.\n",
    "     - `'DOLocationID'`: Dropoff location ID.\n",
    "     - `'tip_amount'`: Amount of tip given.\n",
    "     - `'total_amount'`: Total amount charged.\n",
    "     - `'congestion_surcharge'`: Additional congestion surcharge.\n",
    "   - Filters the DataFrame to include only these columns.\n",
    "\n",
    "3. **Output**:\n",
    "   - A cleaned DataFrame containing only the specified columns.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - If an error occurs during processing, the function prints the error message and returns `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parquet_columns(parquet_file):\n",
    "    \"\"\"\n",
    "    Read a Parquet file, keep only relevant columns, and return the cleaned DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - parquet_file (str): Path to the Parquet file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Parquet file\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "\n",
    "        # Specify the columns to keep\n",
    "        columns_to_keep = [\n",
    "            'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "            'passenger_count', 'trip_distance',\n",
    "            'PULocationID', 'DOLocationID', 'tip_amount', 'total_amount', 'congestion_surcharge'\n",
    "        ]\n",
    "\n",
    "        # Filter the DataFrame to keep only the specified columns\n",
    "        df_cleaned = df[columns_to_keep]\n",
    "        return df_cleaned\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the Parquet file: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2020-02-19 19:37:08   2020-02-19 19:42:59              1.0           1.24   \n",
      "1  2020-02-27 12:17:32   2020-02-27 12:26:00              1.0           0.86   \n",
      "2  2020-02-06 14:38:29   2020-02-06 14:44:46              1.0           1.40   \n",
      "3  2020-02-01 12:10:41   2020-02-01 12:22:25              1.0           1.80   \n",
      "4  2020-02-14 19:18:29   2020-02-14 19:37:45              3.0           1.85   \n",
      "5  2020-02-17 16:02:28   2020-02-17 16:09:40              1.0           1.90   \n",
      "6  2020-02-08 12:38:37   2020-02-08 12:51:12              1.0           1.74   \n",
      "7  2020-02-23 19:40:40   2020-02-23 19:48:55              1.0           1.31   \n",
      "8  2020-02-01 04:43:14   2020-02-01 04:53:06              1.0           2.10   \n",
      "9  2020-02-06 17:27:39   2020-02-06 17:43:09              1.0           3.13   \n",
      "\n",
      "   PULocationID  DOLocationID  tip_amount  total_amount  congestion_surcharge  \n",
      "0           237           236        0.00         10.30                   2.5  \n",
      "1           237           237        0.00         10.30                   2.5  \n",
      "2            48           143        2.00         12.30                   2.5  \n",
      "3           142           140        2.56         15.36                   2.5  \n",
      "4           246           137        3.36         20.16                   2.5  \n",
      "5            68            50        2.15         12.95                   2.5  \n",
      "6           237           164        2.56         15.36                   2.5  \n",
      "7            48           143        0.00         10.80                   2.5  \n",
      "8            79           164        2.00         15.30                   2.5  \n",
      "9           143            68        4.33         21.63                   2.5  \n"
     ]
    }
   ],
   "source": [
    "file= r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-02_sample.parquet\"\n",
    "clean = clean_parquet_columns(file)\n",
    "print(clean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c92ba-568e-463a-9e1f-e184d3ca41c9",
   "metadata": {},
   "source": [
    "### Step 2: Define the Normalization Function\n",
    "\n",
    "The `normalize_column_names` function modifies column names in the following ways:\n",
    "1. **Convert to Lowercase**:\n",
    "   - Ensures all column names are in lowercase for uniformity.\n",
    "2. **Remove Special Characters**:\n",
    "   - Strips out non-alphanumeric characters except for underscores.\n",
    "3. **Replace Spaces with Underscores**:\n",
    "   - Converts spaces and other whitespace to underscores.\n",
    "4. **Strip Leading and Trailing Spaces**:\n",
    "   - Removes any extra spaces around column names.\n",
    "\n",
    "#### **Input**:\n",
    "- `df`: A Pandas DataFrame with column names to be normalized.\n",
    "\n",
    "#### **Output**:\n",
    "- A DataFrame with normalized column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "344c5e3f-4ab8-4762-8cec-6e474cd01b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column_names(df):\n",
    "    \"\"\"\n",
    "    Normalize column names in a DataFrame by:\n",
    "    - Converting to lowercase\n",
    "    - Replacing spaces and special characters with underscores\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with normalized column names.\n",
    "    \"\"\"\n",
    "    # Normalize column names\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()               # Remove leading/trailing spaces\n",
    "        .str.lower()               # Convert to lowercase\n",
    "        .str.replace(r'[^\\w\\s]', '', regex=True)  # Remove special characters\n",
    "        .str.replace(r'\\s+', '_', regex=True)     # Replace spaces with underscores\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "475c4582-4d59-493f-a5c8-3e25679698fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2020-02-19 19:37:08   2020-02-19 19:42:59              1.0           1.24   \n",
      "1  2020-02-27 12:17:32   2020-02-27 12:26:00              1.0           0.86   \n",
      "2  2020-02-06 14:38:29   2020-02-06 14:44:46              1.0           1.40   \n",
      "3  2020-02-01 12:10:41   2020-02-01 12:22:25              1.0           1.80   \n",
      "4  2020-02-14 19:18:29   2020-02-14 19:37:45              3.0           1.85   \n",
      "5  2020-02-17 16:02:28   2020-02-17 16:09:40              1.0           1.90   \n",
      "6  2020-02-08 12:38:37   2020-02-08 12:51:12              1.0           1.74   \n",
      "7  2020-02-23 19:40:40   2020-02-23 19:48:55              1.0           1.31   \n",
      "8  2020-02-01 04:43:14   2020-02-01 04:53:06              1.0           2.10   \n",
      "9  2020-02-06 17:27:39   2020-02-06 17:43:09              1.0           3.13   \n",
      "\n",
      "   pulocationid  dolocationid  tip_amount  total_amount  congestion_surcharge  \n",
      "0           237           236        0.00         10.30                   2.5  \n",
      "1           237           237        0.00         10.30                   2.5  \n",
      "2            48           143        2.00         12.30                   2.5  \n",
      "3           142           140        2.56         15.36                   2.5  \n",
      "4           246           137        3.36         20.16                   2.5  \n",
      "5            68            50        2.15         12.95                   2.5  \n",
      "6           237           164        2.56         15.36                   2.5  \n",
      "7            48           143        0.00         10.80                   2.5  \n",
      "8            79           164        2.00         15.30                   2.5  \n",
      "9           143            68        4.33         21.63                   2.5  \n"
     ]
    }
   ],
   "source": [
    "clean1 = normalize_column_names(clean)\n",
    "print(clean1.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e7aa7-fbbd-4c02-a193-4271e3b4af65",
   "metadata": {},
   "source": [
    "### Step 3: Define the Processing Function\n",
    "This section explains how to process a taxi trip dataset by cleaning, mapping location IDs to geographic coordinates, and filtering the data based on a bounding box.\n",
    "The `process_taxi_data_from_df` function processes a taxi trip dataset in the following steps:\n",
    "\n",
    "\n",
    "1. **Processing Steps**:\n",
    "   - **Normalize Column Names**:\n",
    "     - Standardizes column names to lowercase and replaces special characters with underscores.\n",
    "   - **Filter Invalid Location IDs**:\n",
    "     - Removes rows where `PULocationID` or `DOLocationID` is missing or outside the valid range (1–263).\n",
    "   - **Map Location IDs to Coordinates**:\n",
    "     - Maps pickup and dropoff location IDs to their respective latitude and longitude using the Taxi Zone GeoDataFrame.\n",
    "   - **Split Coordinates**:\n",
    "     - Adds separate columns for pickup and dropoff latitude/longitude.\n",
    "   - **Filter Rows by Bounding Box**:\n",
    "     - Keeps rows with valid pickup and dropoff coordinates within the defined geographic bounding box.\n",
    "\n",
    "2. **Output**:\n",
    "   - A cleaned and filtered DataFrame ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1f7babd-c468-4810-a790-2c4e059bd337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_taxi_data_from_df(df, loaded_taxi_zones, lat_min=40.560445, lon_min=-74.242330, lat_max=40.908524, lon_max=-73.717047):\n",
    "    \"\"\"\n",
    "    Process a DataFrame by cleaning and filtering based on PULocationID and DOLocationID.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        loaded_taxi_zones (gpd.GeoDataFrame): GeoDataFrame containing Taxi Zone data.\n",
    "        lat_min (float): Minimum latitude for the bounding box.\n",
    "        lon_min (float): Minimum longitude for the bounding box.\n",
    "        lat_max (float): Maximum latitude for the bounding box.\n",
    "        lon_max (float): Maximum longitude for the bounding box.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with valid coordinates.\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid warnings\n",
    "    df = df.copy()\n",
    "    print(f\"Processing DataFrame with {len(df)} rows.\")\n",
    "\n",
    "    # Normalize column names\n",
    "    df = normalize_column_names(df)\n",
    "\n",
    "    # Step 1: Drop rows where PULocationID or DOLocationID is NaN or not in range 1-263\n",
    "    valid_ids = range(1, 264)\n",
    "    df = df[\n",
    "        df['pulocationid'].isin(valid_ids) &\n",
    "        df['dolocationid'].isin(valid_ids)\n",
    "    ]\n",
    "    print(f\"After removing invalid location IDs: {len(df)} rows remaining.\")\n",
    "\n",
    "    # Step 2: Map PULocationID and DOLocationID to coordinates\n",
    "    def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "        try:\n",
    "            zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "            if zone.empty:\n",
    "                return None\n",
    "            centroid = zone.iloc[0].geometry.centroid\n",
    "            return centroid.y, centroid.x\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error looking up coordinates for Zone ID {zone_loc_id}: {e}\")\n",
    "\n",
    "    df.loc[:, 'pickup_coords'] = df['pulocationid'].apply(partial(lookup_coords_for_taxi_zone_id, loaded_taxi_zones=loaded_taxi_zones))\n",
    "    df.loc[:, 'dropoff_coords'] = df['dolocationid'].apply(partial(lookup_coords_for_taxi_zone_id, loaded_taxi_zones=loaded_taxi_zones))\n",
    "\n",
    "    # Split coordinates into separate latitude and longitude columns\n",
    "    pickup_coords_df = pd.DataFrame(df['pickup_coords'].tolist(), columns=['pickup_latitude', 'pickup_longitude'])\n",
    "    dropoff_coords_df = pd.DataFrame(df['dropoff_coords'].tolist(), columns=['dropoff_latitude', 'dropoff_longitude'])\n",
    "\n",
    "    df = pd.concat([df, pickup_coords_df, dropoff_coords_df], axis=1)\n",
    "\n",
    "    # Drop rows where mapping failed (NaN coordinates)\n",
    "    df = df.dropna(subset=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'])\n",
    "    print(f\"After mapping location IDs to coordinates: {len(df)} rows remaining.\")\n",
    "\n",
    "    # Step 3: Filter rows based on the bounding box\n",
    "    df = df[\n",
    "        (df['pickup_latitude'].between(lat_min, lat_max)) &\n",
    "        (df['pickup_longitude'].between(lon_min, lon_max)) &\n",
    "        (df['dropoff_latitude'].between(lat_min, lat_max)) &\n",
    "        (df['dropoff_longitude'].between(lon_min, lon_max))\n",
    "    ]\n",
    "    print(f\"After filtering by bounding box: {len(df)} rows remaining.\")\n",
    "\n",
    "    # Drop intermediate columns if not needed\n",
    "    df = df.drop(columns=['pickup_coords', 'dropoff_coords'])\n",
    "\n",
    "    print(\"Processing completed.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed261d-c4f2-46bd-a6fa-291739ad0700",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 4: Define Total_clean Function\n",
    "\n",
    "The `total_clean_taxi_data` function combines previously defined steps into a streamlined process for data cleaning and filtering.\n",
    "\n",
    "#### **Processing Steps**:\n",
    "1. **Clean Parquet Columns**:\n",
    "   - Reads the Parquet file and retains only the relevant columns, such as pickup/dropoff timestamps, passenger count, trip distance, and location IDs.\n",
    "2. **Normalize Column Names**:\n",
    "   - Converts column names to lowercase and replaces spaces/special characters with underscores for consistency.\n",
    "3. **Filter and Map Data**:\n",
    "   - Maps location IDs to geographic coordinates, filters invalid rows, and applies a bounding box to keep rows within the NYC area.\n",
    "\n",
    "#### **Output**:\n",
    "- A cleaned and filtered DataFrame ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb0fead3-650d-4e65-a6ef-d765c97b835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a reference to a function that I wrote separately in the three steps above, and put it all together.\n",
    "def total_clean_taxi_data(file, taxi_zones_gdf):\n",
    "    \"\"\"\n",
    "    Process a Parquet file by cleaning, normalizing, and filtering data.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): Path to the Parquet file.\n",
    "        taxi_zones_gdf (gpd.GeoDataFrame): GeoDataFrame containing Taxi Zone data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed and filtered DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing file: {file}\")\n",
    "\n",
    "        # Step 1: Clean the Parquet columns\n",
    "        cleaned_df = clean_parquet_columns(file)\n",
    "        if cleaned_df is None:\n",
    "            raise ValueError(\"Cleaning Parquet columns failed.\")\n",
    "\n",
    "        # Step 2: Normalize column names\n",
    "        normalized_df = normalize_column_names(cleaned_df)\n",
    "\n",
    "        # Step 3: Process the taxi data\n",
    "        processed_df = process_taxi_data_from_df(normalized_df, taxi_zones_gdf)\n",
    "\n",
    "        print(\"Processing completed.\")\n",
    "        return processed_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1878a4b-c7aa-47f6-af78-a7d0f8110318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-09_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 382 rows remaining.\n",
      "After mapping location IDs to coordinates: 382 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2021-09-03 13:12:41   2021-09-03 13:19:43              1.0           1.01   \n",
      "1  2021-09-14 17:57:35   2021-09-14 18:19:34              1.0           2.67   \n",
      "2  2021-09-18 06:47:25   2021-09-18 06:52:03              1.0           1.04   \n",
      "3  2021-09-20 10:32:05   2021-09-20 10:40:46              1.0           1.04   \n",
      "4  2021-09-25 11:59:50   2021-09-25 12:00:08              1.0           0.00   \n",
      "5  2021-09-19 10:56:31   2021-09-19 11:07:40              2.0           2.00   \n",
      "6  2021-09-01 16:14:18   2021-09-01 16:22:33              1.0           1.00   \n",
      "7  2021-09-22 17:45:40   2021-09-22 17:54:51              1.0           1.32   \n",
      "8  2021-09-25 20:29:40   2021-09-25 20:35:04              1.0           0.75   \n",
      "9  2021-09-02 02:43:24   2021-09-02 02:53:43              1.0           3.09   \n",
      "\n",
      "   pulocationid  dolocationid  tip_amount  total_amount  congestion_surcharge  \\\n",
      "0         237.0          43.0        1.86         11.16                   2.5   \n",
      "1          79.0         246.0        1.00         20.30                   2.5   \n",
      "2         237.0         236.0        0.00          9.30                   2.5   \n",
      "3         236.0         237.0        2.06         12.36                   2.5   \n",
      "4         141.0         141.0        0.00          5.80                   2.5   \n",
      "5          68.0         230.0        0.00         12.80                   2.5   \n",
      "6         236.0         141.0        2.03         13.33                   2.5   \n",
      "7         142.0         237.0        0.00         11.80                   2.5   \n",
      "8          90.0          68.0        1.86         11.16                   2.5   \n",
      "9         148.0         255.0        3.06         18.36                   2.5   \n",
      "\n",
      "   pickup_latitude  pickup_longitude  dropoff_latitude  dropoff_longitude  \n",
      "0        40.768615        -73.965635         40.782478         -73.965554  \n",
      "1        40.727620        -73.985937         40.753309         -74.004015  \n",
      "2        40.768615        -73.965635         40.780436         -73.957012  \n",
      "3        40.780436        -73.957012         40.768615         -73.965635  \n",
      "4        40.766948        -73.959635         40.766948         -73.959635  \n",
      "5        40.748428        -73.999917         40.759818         -73.984196  \n",
      "6        40.780436        -73.957012         40.766948         -73.959635  \n",
      "7        40.773633        -73.981532         40.768615         -73.965635  \n",
      "8        40.742279        -73.996971         40.748428         -73.999917  \n",
      "9        40.718938        -73.990896         40.718804         -73.957418  \n"
     ]
    }
   ],
   "source": [
    "file= r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-09_sample.parquet\"\n",
    "ok= total_clean_taxi_data(file, taxi_zones_gdf)\n",
    "print(ok.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4a660-00a8-4376-9762-9d2199380f28",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 5: Define the merge files Function `process_and_merge_files`\n",
    "\n",
    "The `process_and_merge_files` function combines multiple DataFrames generated from taxi trip data files into a single consolidated DataFrame.\n",
    "\n",
    "\n",
    "#### **Processing Steps**:\n",
    "1. **Iterate Through Files**:\n",
    "   - Reads each file in the directory.\n",
    "   - Ensures the file is valid before processing.\n",
    "2. **Apply Cleaning and Filtering**:\n",
    "   - Uses the `total_clean_taxi_data` function to clean and filter each file.\n",
    "   - Appends the processed data to a list.\n",
    "3. **Merge DataFrames**:\n",
    "   - Combines all processed DataFrames into a single DataFrame.\n",
    "   - Ensures unique indices by resetting the index during concatenation.\n",
    "4. **Output**:\n",
    "   - Returns the merged DataFrame containing all processed rows.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3469adbc-045c-41c9-ba32-a71d0ce9ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all dataframe that generate from parquet \n",
    "def process_and_merge_files(directory, taxi_zones_gdf):\n",
    "    \"\"\"\n",
    "    Process all files in a directory using total_clean_taxi_data, \n",
    "    and merge the resulting DataFrames where each processed file \n",
    "    contributes one row to the final DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing the files.\n",
    "        taxi_zones_gdf (GeoDataFrame): The taxi zones GeoDataFrame required by total_clean_taxi_data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A merged DataFrame with unique indices.\n",
    "    \"\"\"\n",
    "    all_data = []  # List to collect processed DataFrames\n",
    "    \n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.isfile(file_path):  # Ensure it's a file\n",
    "            try:\n",
    "                # Process the file using total_clean_taxi_data\n",
    "                processed_data = total_clean_taxi_data(file_path, taxi_zones_gdf)\n",
    "                if isinstance(processed_data, pd.DataFrame):\n",
    "                    # Add the resulting DataFrame with reset index to avoid duplicate indices\n",
    "                    all_data.append(processed_data.reset_index(drop=True))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "    \n",
    "    # Combine all DataFrames into a single DataFrame\n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)  # Ensure final DataFrame has unique indices\n",
    "    else:\n",
    "        final_df = pd.DataFrame()  # Return an empty DataFrame if no files were processed\n",
    "    \n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e2d70f2-3d72-4602-939b-337a3151e82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-01_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-02_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 382 rows remaining.\n",
      "After mapping location IDs to coordinates: 382 rows remaining.\n",
      "After filtering by bounding box: 382 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-03_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-04_sample.parquet\n",
      "Processing DataFrame with 384 rows.\n",
      "After removing invalid location IDs: 383 rows remaining.\n",
      "After mapping location IDs to coordinates: 383 rows remaining.\n",
      "After filtering by bounding box: 383 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-05_sample.parquet\n",
      "Processing DataFrame with 384 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-06_sample.parquet\n",
      "Processing DataFrame with 384 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-07_sample.parquet\n",
      "Processing DataFrame with 384 rows.\n",
      "After removing invalid location IDs: 382 rows remaining.\n",
      "After mapping location IDs to coordinates: 382 rows remaining.\n",
      "After filtering by bounding box: 382 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-08_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-09_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-10_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-11_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-12_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-01_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-02_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 374 rows remaining.\n",
      "After mapping location IDs to coordinates: 374 rows remaining.\n",
      "After filtering by bounding box: 374 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-03_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-04_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-05_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-06_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-07_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-08_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-09_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 382 rows remaining.\n",
      "After mapping location IDs to coordinates: 382 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-10_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-11_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-12_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 372 rows remaining.\n",
      "After mapping location IDs to coordinates: 372 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-01_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 382 rows remaining.\n",
      "After mapping location IDs to coordinates: 382 rows remaining.\n",
      "After filtering by bounding box: 382 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-02_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 373 rows remaining.\n",
      "After mapping location IDs to coordinates: 373 rows remaining.\n",
      "After filtering by bounding box: 373 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-03_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-04_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 372 rows remaining.\n",
      "After mapping location IDs to coordinates: 372 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-05_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-06_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-07_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 375 rows remaining.\n",
      "After mapping location IDs to coordinates: 375 rows remaining.\n",
      "After filtering by bounding box: 374 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-08_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-09_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-10_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-11_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 372 rows remaining.\n",
      "After mapping location IDs to coordinates: 372 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-12_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-01_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-02_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-03_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-04_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-05_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-06_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-07_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 375 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-08_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-09_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-10_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-11_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-12_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-01_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-02_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-03_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-04_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 383 rows remaining.\n",
      "After mapping location IDs to coordinates: 383 rows remaining.\n",
      "After filtering by bounding box: 383 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-05_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-06_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-07_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-08_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "directory= r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\"\n",
    "taxi_data=process_and_merge_files(directory, taxi_zones_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "198a78c6-9de2-4330-9de6-e613f69a0721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0   2020-01-25 10:49:58   2020-01-25 11:07:35              1.0           3.28   \n",
      "1   2020-01-15 07:30:08   2020-01-15 07:40:01              1.0           1.75   \n",
      "2   2020-01-09 06:29:09   2020-01-09 06:35:44              1.0           0.87   \n",
      "3   2020-01-26 12:24:04   2020-01-26 12:29:15              2.0           0.98   \n",
      "4   2020-01-30 07:57:53   2020-01-30 08:10:19              1.0           1.30   \n",
      "..                  ...                   ...              ...            ...   \n",
      "95  2020-01-02 18:44:02   2020-01-02 19:04:20              1.0           1.98   \n",
      "96  2020-01-06 15:20:04   2020-01-06 15:25:53              1.0           0.80   \n",
      "97  2020-01-30 14:18:00   2020-01-30 14:58:52              1.0          10.76   \n",
      "98  2020-01-02 08:19:56   2020-01-02 08:30:49              5.0           1.62   \n",
      "99  2020-01-21 21:51:49   2020-01-21 21:53:33              1.0           0.28   \n",
      "\n",
      "    pulocationid  dolocationid  tip_amount  total_amount  \\\n",
      "0          142.0         246.0        1.70         19.00   \n",
      "1          238.0         166.0        1.20         13.00   \n",
      "2          100.0         164.0        0.00          8.80   \n",
      "3          161.0          43.0        0.00          8.80   \n",
      "4          229.0         262.0        2.45         14.75   \n",
      "..           ...           ...         ...           ...   \n",
      "95         162.0         141.0        0.00         17.80   \n",
      "96          79.0         211.0        1.85         11.15   \n",
      "97         234.0         138.0        4.59         50.51   \n",
      "98         107.0         100.0        1.00         13.30   \n",
      "99         141.0         237.0        0.00          7.30   \n",
      "\n",
      "    congestion_surcharge  pickup_latitude  pickup_longitude  dropoff_latitude  \\\n",
      "0                    2.5        40.791705        -73.973049         40.809457   \n",
      "1                    2.5        40.753513        -73.988787         40.748575   \n",
      "2                    2.5        40.758028        -73.977698         40.782478   \n",
      "3                    2.5        40.756729        -73.965146         40.775932   \n",
      "4                    2.5        40.756688        -73.972356         40.758028   \n",
      "..                   ...              ...               ...               ...   \n",
      "95                   2.5        40.727620        -73.985937         40.723888   \n",
      "96                   2.5        40.740337        -73.990458         40.774376   \n",
      "97                   2.5        40.736824        -73.984052         40.753513   \n",
      "98                   2.5        40.766948        -73.959635         40.768615   \n",
      "99                   2.5        40.778766        -73.951010         40.782478   \n",
      "\n",
      "    dropoff_longitude  \n",
      "0          -73.961764  \n",
      "1          -73.985156  \n",
      "2          -73.965554  \n",
      "3          -73.946510  \n",
      "4          -73.977698  \n",
      "..                ...  \n",
      "95         -74.001538  \n",
      "96         -73.873629  \n",
      "97         -73.988787  \n",
      "98         -73.965635  \n",
      "99         -73.965554  \n",
      "\n",
      "[100 rows x 13 columns]\n",
      "Total number of rows: 21182\n"
     ]
    }
   ],
   "source": [
    "print(taxi_data.head(100))\n",
    "total_rows = taxi_data.shape[0]\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea95130a-3640-477f-97ac-a7c1ac47444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the taxi_data DataFrame as a CSV file\n",
    "taxi_data.to_csv(\"taxi_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85278583-9c25-462c-8231-55ba656a57b0",
   "metadata": {},
   "source": [
    "### Step 1: Use the previously defined function `sample_parquet_file`\n",
    "- Because the dataset is so large, we can only split each year's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "07574983-f41d-4cd6-8f70-489493089b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-01.parquet\n",
      "Population size: 20569368, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-02.parquet\n",
      "Population size: 21725100, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-03.parquet\n",
      "Population size: 13392928, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-04.parquet\n",
      "Population size: 4312909, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-05.parquet\n",
      "Population size: 6089999, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-06.parquet\n",
      "Population size: 7555193, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-07.parquet\n",
      "Population size: 9958454, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-08.parquet\n",
      "Population size: 11096852, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-09.parquet\n",
      "Population size: 12106669, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-10.parquet\n",
      "Population size: 13268411, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-11.parquet\n",
      "Population size: 11596865, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\\fhvhv_tripdata_2020-12.parquet\n",
      "Population size: 11637123, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2020-12_sample.parquet\n"
     ]
    }
   ],
   "source": [
    "input_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2020\"\n",
    "output_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\"\n",
    "\n",
    "# Apply sampling to all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1b3d85ff-313c-41a2-9a46-261a9a2bb10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-01.parquet\n",
      "Population size: 11908468, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-02.parquet\n",
      "Population size: 11613942, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-03.parquet\n",
      "Population size: 14227393, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-04.parquet\n",
      "Population size: 14111371, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-05.parquet\n",
      "Population size: 14719171, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-06.parquet\n",
      "Population size: 14961892, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-07.parquet\n",
      "Population size: 15027174, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-08.parquet\n",
      "Population size: 14499696, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-09.parquet\n",
      "Population size: 14886055, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-10.parquet\n",
      "Population size: 16545356, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-11.parquet\n",
      "Population size: 16041639, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\\fhvhv_tripdata_2021-12.parquet\n",
      "Population size: 16054495, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2021-12_sample.parquet\n"
     ]
    }
   ],
   "source": [
    "input_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2021\"\n",
    "output_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\"\n",
    "\n",
    "# Apply sampling to all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3c5e4df8-f294-4fb6-855a-d30bb806f470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-01.parquet\n",
      "Population size: 14751591, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-02.parquet\n",
      "Population size: 16019283, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-03.parquet\n",
      "Population size: 18453548, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-04.parquet\n",
      "Population size: 17752561, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-05.parquet\n",
      "Population size: 18157335, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-06.parquet\n",
      "Population size: 17780075, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-07.parquet\n",
      "Population size: 17464619, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-08.parquet\n",
      "Population size: 17185687, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-09.parquet\n",
      "Population size: 17793551, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-10.parquet\n",
      "Population size: 19306090, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-11.parquet\n",
      "Population size: 18085896, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\\fhvhv_tripdata_2022-12.parquet\n",
      "Population size: 19665847, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2022-12_sample.parquet\n"
     ]
    }
   ],
   "source": [
    "input_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2022\"\n",
    "output_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\"\n",
    "\n",
    "# Apply sampling to all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3c9dcc4b-5173-4833-8016-5e9d714c586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-01.parquet\n",
      "Population size: 18479031, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-02.parquet\n",
      "Population size: 17960971, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-03.parquet\n",
      "Population size: 20413539, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-04.parquet\n",
      "Population size: 19144903, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-05.parquet\n",
      "Population size: 19847676, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-06.parquet\n",
      "Population size: 19366619, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-07.parquet\n",
      "Population size: 19132131, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-08.parquet\n",
      "Population size: 18322150, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-09.parquet\n",
      "Population size: 19851123, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-10.parquet\n",
      "Population size: 20186330, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-11.parquet\n",
      "Population size: 19269250, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\\fhvhv_tripdata_2023-12.parquet\n",
      "Population size: 20516297, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2023-12_sample.parquet\n"
     ]
    }
   ],
   "source": [
    "input_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2023\"\n",
    "output_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\"\n",
    "\n",
    "# Apply sampling to all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f7ebb725-b910-4c8d-855c-863e3fdab1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-01.parquet\n",
      "Population size: 19663930, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2024-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-02.parquet\n",
      "Population size: 19359148, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2024-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-03.parquet\n",
      "Population size: 21280788, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2024-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-04.parquet\n",
      "Population size: 19733038, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2024-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-05.parquet\n",
      "Population size: 20704538, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2024-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-06.parquet\n",
      "Population size: 20123226, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2024-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-07.parquet\n",
      "Population size: 19182934, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2024-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\\fhvhv_tripdata_2024-08.parquet\n",
      "Population size: 19128392, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\\fhvhv_tripdata_2024-08_sample.parquet\n"
     ]
    }
   ],
   "source": [
    "input_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV/2024\"\n",
    "output_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\"\n",
    "\n",
    "# Apply sampling to all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb369d0-36d8-4c46-a066-c686b2357f1c",
   "metadata": {},
   "source": [
    "### Step 2: `clean1_parquet_columns`\n",
    "\n",
    "The function reads a Parquet file and filters the dataset to keep only the specified columns.\n",
    "\n",
    "\n",
    "#### **Process**:\n",
    "1. **Load Data**:\n",
    "   - Reads the Parquet file into a Pandas DataFrame.\n",
    "2. **Select Relevant Columns**:\n",
    "   - Retains only the following columns:\n",
    "     - `pickup_datetime`: The pickup date and time.\n",
    "     - `dropoff_datetime`: The dropoff date and time.\n",
    "     - `PULocationID`: Pickup location ID.\n",
    "     - `DOLocationID`: Dropoff location ID.\n",
    "     - `trip_miles`: Distance of the trip in miles.\n",
    "     - `trip_time`: Duration of the trip.\n",
    "     - `base_passenger_fare`: Base fare charged to the passenger.\n",
    "     - `tolls`: Toll charges.\n",
    "     - `bcf`: Black Car Fund surcharge.\n",
    "     - `sales_tax`: Sales tax for the trip.\n",
    "     - `congestion_surcharge`: Congestion pricing surcharge.\n",
    "     - `airport_fee`: Additional airport fee (if applicable).\n",
    "     - `tips`: Tips given by the passenger.\n",
    "3. **Error Handling**:\n",
    "   - If an error occurs (e.g., missing columns, file not found), the function prints an error message and returns `None`.\n",
    "\n",
    "#### **Output**:\n",
    "- A DataFrame containing only the specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean1_parquet_columns(parquet_file):\n",
    "    \"\"\"\n",
    "    Read a Parquet file, keep only relevant columns, and return the cleaned DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - parquet_file (str): Path to the Parquet file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Parquet file\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "\n",
    "        # Specify the columns to keep\n",
    "        columns_to_keep = [\n",
    "            'pickup_datetime',\n",
    "            'dropoff_datetime',\n",
    "            'PULocationID',\n",
    "            'DOLocationID',\n",
    "            'trip_miles',\n",
    "            'trip_time',\n",
    "            'base_passenger_fare',\n",
    "            'tolls',\n",
    "            'bcf',\n",
    "            'sales_tax',\n",
    "            'congestion_surcharge',\n",
    "            'airport_fee',\n",
    "            'tips',\n",
    "        ]\n",
    "\n",
    "        # Filter the DataFrame to keep only the specified columns\n",
    "        df_cleaned = df[columns_to_keep]\n",
    "        return df_cleaned\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the Parquet file: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100bf8a-b40d-4012-af61-7c8ebeddbace",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: `filter_hvfhs_license_num`\n",
    "\n",
    "This function filters the input DataFrame to include only rows with a specified HVFHS license number, because only this license number represent Uber \n",
    "#### **Processing Steps**:\n",
    "1. **Filter Rows**:\n",
    "   - Retains rows where the value in the `hvfhs_license_num` column is `'HV0003'`.\n",
    "2. **Error Handling**:\n",
    "   - If the column `hvfhs_license_num` is not present in the DataFrame, the function prints an error message and returns the original DataFrame.\n",
    "\n",
    "#### **Output**:\n",
    "- A filtered DataFrame containing only rows where `hvfhs_license_num == 'HV0003'`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hvfhs_license_num(df):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to keep only rows where hvfhs_license_num is 'HV0003'.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The filtered DataFrame containing only rows with hvfhs_license_num == 'HV0003'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter the DataFrame\n",
    "        filtered_df = df[df['hvfhs_license_num'] == 'HV0003']\n",
    "        return filtered_df\n",
    "    except KeyError:\n",
    "        print(\"Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7df0bf-8bff-4cc3-a020-2ee0d246df8c",
   "metadata": {},
   "source": [
    "### Step 4,5,6 : The following appears similar to functions above when processing taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  It's a reference to a function that I wrote separately in the four steps above, and put it all together.\n",
    "def total_process_uber_data(file_path, taxi_zones_gdf):\n",
    "    \"\"\"\n",
    "    Process Uber data by applying a series of transformations:\n",
    "    - Clean relevant columns.\n",
    "    - Normalize column names.\n",
    "    - Filter rows to include only Uber rides.\n",
    "    - Process data with taxi zone geospatial information.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the input Parquet file.\n",
    "    - taxi_zones_gdf (GeoDataFrame): GeoDataFrame containing taxi zone geometries.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Clean and keep relevant columns\n",
    "        Uberclean = clean1_parquet_columns(file_path)\n",
    "        \n",
    "        # Step 2: Normalize column names\n",
    "        Uberclean1 = normalize_column_names(Uberclean)\n",
    "        \n",
    "        # Step 3: Filter out non-Uber rides\n",
    "        Uberclean2 = filter_hvfhs_license_num(Uberclean1)\n",
    "        \n",
    "        # Step 4: Process data with taxi zones\n",
    "        Uberclean3 = process_taxi_data_from_df(Uberclean2, taxi_zones_gdf)\n",
    "        \n",
    "        # Return the final processed DataFrame\n",
    "        return Uberclean3\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Uber data processing: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_merge_files(directory, taxi_zones_gdf):\n",
    "    \"\"\"\n",
    "    Process all files in a directory using total_clean_taxi_data, \n",
    "    and merge the resulting DataFrames where each processed file \n",
    "    contributes one row to the final DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing the files.\n",
    "        taxi_zones_gdf (GeoDataFrame): The taxi zones GeoDataFrame required by total_clean_taxi_data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A merged DataFrame with unique indices.\n",
    "    \"\"\"\n",
    "    all_data = []  # List to collect processed DataFrames\n",
    "    \n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.isfile(file_path):  # Ensure it's a file\n",
    "            try:\n",
    "                # Process the file using total_clean_taxi_data\n",
    "                processed_data = total_process_uber_data(file_path, taxi_zones_gdf)\n",
    "                if isinstance(processed_data, pd.DataFrame):\n",
    "                    # Add the resulting DataFrame with reset index to avoid duplicate indices\n",
    "                    all_data.append(processed_data.reset_index(drop=True))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "    \n",
    "    # Combine all DataFrames into a single DataFrame\n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)  # Ensure final DataFrame has unique indices\n",
    "    else:\n",
    "        final_df = pd.DataFrame()  # Return an empty DataFrame if no files were processed\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 367 rows remaining.\n",
      "After mapping location IDs to coordinates: 367 rows remaining.\n",
      "After filtering by bounding box: 366 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 375 rows remaining.\n",
      "After mapping location IDs to coordinates: 375 rows remaining.\n",
      "After filtering by bounding box: 374 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 375 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 372 rows remaining.\n",
      "After mapping location IDs to coordinates: 372 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 374 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 375 rows remaining.\n",
      "After mapping location IDs to coordinates: 375 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 375 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 375 rows remaining.\n",
      "After filtering by bounding box: 373 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 373 rows remaining.\n",
      "After mapping location IDs to coordinates: 373 rows remaining.\n",
      "After filtering by bounding box: 371 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 374 rows remaining.\n",
      "After mapping location IDs to coordinates: 373 rows remaining.\n",
      "After filtering by bounding box: 373 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 374 rows remaining.\n",
      "After mapping location IDs to coordinates: 374 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 372 rows remaining.\n",
      "After mapping location IDs to coordinates: 371 rows remaining.\n",
      "After filtering by bounding box: 368 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 373 rows remaining.\n",
      "After mapping location IDs to coordinates: 373 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 373 rows remaining.\n",
      "After mapping location IDs to coordinates: 373 rows remaining.\n",
      "After filtering by bounding box: 369 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 365 rows remaining.\n",
      "After mapping location IDs to coordinates: 365 rows remaining.\n",
      "After filtering by bounding box: 362 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 367 rows remaining.\n",
      "After mapping location IDs to coordinates: 367 rows remaining.\n",
      "After filtering by bounding box: 364 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 373 rows remaining.\n",
      "After mapping location IDs to coordinates: 373 rows remaining.\n",
      "After filtering by bounding box: 371 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 368 rows remaining.\n",
      "After mapping location IDs to coordinates: 368 rows remaining.\n",
      "After filtering by bounding box: 366 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 365 rows remaining.\n",
      "After mapping location IDs to coordinates: 365 rows remaining.\n",
      "After filtering by bounding box: 363 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 367 rows remaining.\n",
      "After mapping location IDs to coordinates: 367 rows remaining.\n",
      "After filtering by bounding box: 366 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 365 rows remaining.\n",
      "After mapping location IDs to coordinates: 365 rows remaining.\n",
      "After filtering by bounding box: 362 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 372 rows remaining.\n",
      "After mapping location IDs to coordinates: 372 rows remaining.\n",
      "After filtering by bounding box: 371 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 375 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 366 rows remaining.\n",
      "After mapping location IDs to coordinates: 366 rows remaining.\n",
      "After filtering by bounding box: 364 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 369 rows remaining.\n",
      "After mapping location IDs to coordinates: 368 rows remaining.\n",
      "After filtering by bounding box: 368 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 373 rows remaining.\n",
      "After mapping location IDs to coordinates: 373 rows remaining.\n",
      "After filtering by bounding box: 373 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 368 rows remaining.\n",
      "After mapping location IDs to coordinates: 368 rows remaining.\n",
      "After filtering by bounding box: 368 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 363 rows remaining.\n",
      "After mapping location IDs to coordinates: 363 rows remaining.\n",
      "After filtering by bounding box: 363 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 359 rows remaining.\n",
      "After mapping location IDs to coordinates: 359 rows remaining.\n",
      "After filtering by bounding box: 359 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 366 rows remaining.\n",
      "After mapping location IDs to coordinates: 366 rows remaining.\n",
      "After filtering by bounding box: 366 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 364 rows remaining.\n",
      "After mapping location IDs to coordinates: 364 rows remaining.\n",
      "After filtering by bounding box: 364 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 373 rows remaining.\n",
      "After mapping location IDs to coordinates: 373 rows remaining.\n",
      "After filtering by bounding box: 373 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 371 rows remaining.\n",
      "After mapping location IDs to coordinates: 371 rows remaining.\n",
      "After filtering by bounding box: 368 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 372 rows remaining.\n",
      "After mapping location IDs to coordinates: 372 rows remaining.\n",
      "After filtering by bounding box: 371 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 367 rows remaining.\n",
      "After mapping location IDs to coordinates: 366 rows remaining.\n",
      "After filtering by bounding box: 365 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 370 rows remaining.\n",
      "After mapping location IDs to coordinates: 370 rows remaining.\n",
      "After filtering by bounding box: 368 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 371 rows remaining.\n",
      "After mapping location IDs to coordinates: 371 rows remaining.\n",
      "After filtering by bounding box: 371 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 369 rows remaining.\n",
      "After mapping location IDs to coordinates: 369 rows remaining.\n",
      "After filtering by bounding box: 366 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 360 rows remaining.\n",
      "After mapping location IDs to coordinates: 360 rows remaining.\n",
      "After filtering by bounding box: 358 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 370 rows remaining.\n",
      "After mapping location IDs to coordinates: 369 rows remaining.\n",
      "After filtering by bounding box: 367 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 369 rows remaining.\n",
      "After mapping location IDs to coordinates: 369 rows remaining.\n",
      "After filtering by bounding box: 366 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 367 rows remaining.\n",
      "After mapping location IDs to coordinates: 367 rows remaining.\n",
      "After filtering by bounding box: 367 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 371 rows remaining.\n",
      "After mapping location IDs to coordinates: 371 rows remaining.\n",
      "After filtering by bounding box: 370 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 370 rows remaining.\n",
      "After mapping location IDs to coordinates: 370 rows remaining.\n",
      "After filtering by bounding box: 368 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 374 rows remaining.\n",
      "After mapping location IDs to coordinates: 374 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 374 rows remaining.\n",
      "After mapping location IDs to coordinates: 374 rows remaining.\n",
      "After filtering by bounding box: 371 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 374 rows remaining.\n",
      "After mapping location IDs to coordinates: 374 rows remaining.\n",
      "After filtering by bounding box: 370 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 368 rows remaining.\n",
      "After mapping location IDs to coordinates: 366 rows remaining.\n",
      "After filtering by bounding box: 364 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 363 rows remaining.\n",
      "After mapping location IDs to coordinates: 363 rows remaining.\n",
      "After filtering by bounding box: 363 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 368 rows remaining.\n",
      "After mapping location IDs to coordinates: 367 rows remaining.\n",
      "After filtering by bounding box: 366 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 371 rows remaining.\n",
      "After mapping location IDs to coordinates: 371 rows remaining.\n",
      "After filtering by bounding box: 368 rows remaining.\n",
      "Processing completed.\n",
      "Error: The column 'hvfhs_license_num' does not exist in the DataFrame.\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 374 rows remaining.\n",
      "After mapping location IDs to coordinates: 374 rows remaining.\n",
      "After filtering by bounding box: 371 rows remaining.\n",
      "Processing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wenji\\AppData\\Local\\Temp\\ipykernel_28936\\385201595.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(all_data, ignore_index=True)  # Ensure final DataFrame has unique indices\n"
     ]
    }
   ],
   "source": [
    "directory= r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled Uber\"\n",
    "Uber_data= process_and_merge_files(directory, taxi_zones_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9573e036-8328-4eba-8165-e46a85f53e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pickup_datetime    dropoff_datetime  pulocationid  dolocationid  \\\n",
      "0  2020-01-06 17:55:52 2020-01-06 18:58:16          48.0         132.0   \n",
      "1  2020-01-17 13:34:41 2020-01-17 13:45:42         263.0          43.0   \n",
      "2  2020-01-20 13:09:30 2020-01-20 13:28:12         188.0          63.0   \n",
      "3  2020-01-11 12:56:38 2020-01-11 13:06:01         143.0          50.0   \n",
      "4  2020-01-28 00:04:23 2020-01-28 00:20:18         162.0         160.0   \n",
      "..                 ...                 ...           ...           ...   \n",
      "95 2020-01-18 00:20:30 2020-01-18 00:35:14          85.0         189.0   \n",
      "96 2020-01-27 08:18:44 2020-01-27 08:59:00          41.0         137.0   \n",
      "97 2020-01-19 14:03:57 2020-01-19 14:19:17         237.0         164.0   \n",
      "98 2020-01-01 06:12:35 2020-01-01 06:17:48          79.0           4.0   \n",
      "99 2020-01-14 08:31:04 2020-01-14 08:52:16         129.0         146.0   \n",
      "\n",
      "    trip_miles  trip_time  base_passenger_fare  tolls   bcf  sales_tax  \\\n",
      "0       20.001     3744.0                62.90   5.41  1.71       6.06   \n",
      "1        1.290      661.0                 9.74   0.00  0.00       0.00   \n",
      "2        3.350     1122.0                 7.47   0.00  0.19       0.66   \n",
      "3        1.363      563.0                 8.11   0.00  0.20       0.70   \n",
      "4        6.680      955.0                 5.18   3.06  0.22       0.76   \n",
      "..         ...        ...                  ...    ...   ...        ...   \n",
      "95       3.090      884.0                15.82   0.00  0.40       1.40   \n",
      "96       6.310     2415.0                15.35   0.00  0.00       1.36   \n",
      "97       2.550      920.0                 7.18   0.00  0.00       0.64   \n",
      "98       0.000      313.0                 5.99   0.00  0.15       0.53   \n",
      "99       3.500     1272.0                14.47   0.00  0.36       1.28   \n",
      "\n",
      "    congestion_surcharge  airport_fee  tips  pickup_latitude  \\\n",
      "0                   2.75          NaN   0.0        40.762253   \n",
      "1                   2.75          NaN   0.0        40.778766   \n",
      "2                   0.00          NaN   0.0        40.658744   \n",
      "3                   2.75          NaN   2.0        40.775965   \n",
      "4                   0.75          NaN   0.0        40.756688   \n",
      "..                   ...          ...   ...              ...   \n",
      "95                  0.00          NaN   0.0        40.809457   \n",
      "96                  0.75          NaN   0.0        40.599954   \n",
      "97                  0.75          NaN   0.0        40.828988   \n",
      "98                  0.75          NaN   0.0        40.711596   \n",
      "99                  0.00          NaN   0.0        40.706808   \n",
      "\n",
      "    pickup_longitude  dropoff_latitude  dropoff_longitude  \n",
      "0         -73.989845         40.646985         -73.786533  \n",
      "1         -73.951010         40.782478         -73.965554  \n",
      "2         -73.947442         40.683840         -73.878173  \n",
      "3         -73.987646         40.766238         -73.995135  \n",
      "4         -73.972356         40.718337         -73.880051  \n",
      "..               ...               ...                ...  \n",
      "95        -73.961764         40.775932         -73.946510  \n",
      "96        -73.964334         40.576961         -73.987943  \n",
      "97        -73.924409         40.807347         -73.916822  \n",
      "98        -73.808729         40.748428         -73.999917  \n",
      "99        -74.007496         40.749914         -73.970443  \n",
      "\n",
      "[100 rows x 17 columns]\n",
      "Total number of rows: 20664\n"
     ]
    }
   ],
   "source": [
    "print(Uber_data.head(100))\n",
    "total_rows = Uber_data.shape[0]\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc51aeb1-fc71-4aa7-91d8-c8f7e1a892e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the taxi_data DataFrame as a CSV file\n",
    "Uber_data.to_csv(\"Uber_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "When preparing the weather data, we don't need to go to the website to search, and grab the link, we can use the Google Drive given by the teacher to download the data directly, and we can use the Google Drive to download the data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    \"\"\"\n",
    "    Get a list of all CSV files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of file paths for all CSV files in the directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.isdir(directory):\n",
    "            raise FileNotFoundError(f\"The specified directory does not exist: {directory}\")\n",
    "        \n",
    "        # Get all CSV files in the directory\n",
    "        csv_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No CSV files found in the directory: {directory}\")\n",
    "        \n",
    "        return csv_files\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting CSV files: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\wenji\\\\OneDrive\\\\Project\\\\downloads weather\\\\2020_weather.csv', 'C:\\\\Users\\\\wenji\\\\OneDrive\\\\Project\\\\downloads weather\\\\2021_weather.csv', 'C:\\\\Users\\\\wenji\\\\OneDrive\\\\Project\\\\downloads weather\\\\2022_weather.csv', 'C:\\\\Users\\\\wenji\\\\OneDrive\\\\Project\\\\downloads weather\\\\2023_weather.csv', 'C:\\\\Users\\\\wenji\\\\OneDrive\\\\Project\\\\downloads weather\\\\2024_weather.csv']\n"
     ]
    }
   ],
   "source": [
    "# Example directory path\n",
    "weather_csv_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\"\n",
    "\n",
    "# Get all CSV file paths\n",
    "csv_files = get_all_weather_csvs(weather_csv_directory)\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d508e87-8462-4f8b-addb-37ee261129df",
   "metadata": {},
   "source": [
    "### Step 1: List All CSV Files in a Directory\n",
    "\n",
    "The `get_all_weather_csvs` function identifies all `.csv` files in the specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Read a CSV file into a Pandas DataFrame with improved error handling.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the CSV data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file with low_memory=False\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        print(f\"Successfully loaded CSV: {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"File is empty: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CSV: C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\\2024_weather.csv\n",
      "Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'NAME',\n",
      "       'REPORT_TYPE', 'SOURCE', 'HourlyAltimeterSetting',\n",
      "       'HourlyDewPointTemperature',\n",
      "       ...\n",
      "       'BackupDirection', 'BackupDistance', 'BackupDistanceUnit',\n",
      "       'BackupElements', 'BackupElevation', 'BackupEquipment',\n",
      "       'BackupLatitude', 'BackupLongitude', 'BackupName',\n",
      "       'WindEquipmentChangeDate'],\n",
      "      dtype='object', length=125)\n"
     ]
    }
   ],
   "source": [
    "# Example CSV file path\n",
    "csv_file_path = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\\2024_weather.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = read_csv_to_dataframe(csv_file_path)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4fa530-1128-4ab9-9c4d-307acf372bf9",
   "metadata": {},
   "source": [
    "### Step 2:  `load_and_clean_weather_data`\n",
    "\n",
    "The  function streamlines the process of filtering and cleaning weather data.\n",
    "\n",
    "#### **Input Parameters**:\n",
    "1. **`csv_file_path`**:\n",
    "   - Path to the weather CSV file to be processed.\n",
    "\n",
    "#### **Processing Steps**:\n",
    "1. **Load CSV Data**:\n",
    "   - Reads the CSV file into a Pandas DataFrame using `pd.read_csv()`.\n",
    "   - Prints a success message if the file is loaded successfully.\n",
    "2. **Filter Useful Columns**:\n",
    "   - Retains only the specified weather-related columns:\n",
    "     - `DATE`: Date of observation.\n",
    "     - `HourlyPrecipitation`: Hourly precipitation levels.\n",
    "     - `HourlyWindSpeed`: Hourly wind speeds.\n",
    "     - `DailySnowfall`: Daily snowfall amounts.\n",
    "     - `DailyAverageWindSpeed`: Daily average wind speed.\n",
    "     - `DailyPrecipitation`: Daily precipitation levels.\n",
    "3. **Drop Columns with All Missing Values**:\n",
    "   - Removes columns where all values are `NaN`.\n",
    "   - Prints the number of retained columns after cleaning.\n",
    "4. **Error Handling**:\n",
    "   - Handles file not found errors (`FileNotFoundError`).\n",
    "   - Handles missing columns errors (`KeyError`).\n",
    "   - Handles other unexpected errors.\n",
    "\n",
    "#### **Output**:\n",
    "- A cleaned DataFrame containing only the useful weather-related columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain useful columns as required\n",
    "def load_and_clean_weather_data(csv_file_path):\n",
    "    \"\"\"\n",
    "    Load a weather CSV file, filter for useful columns, remove columns with all NaN values,\n",
    "    and return a cleaned DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        csv_file_path (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame containing only useful weather-related columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_file_path, low_memory=False)\n",
    "        print(f\"Successfully loaded CSV: {csv_file_path}\")\n",
    "\n",
    "        # Define the useful columns based on the inspection\n",
    "        useful_columns = [\n",
    "            'DATE', 'HourlyPrecipitation', 'HourlyWindSpeed', \n",
    "            'DailySnowfall', 'DailyAverageWindSpeed', 'DailyPrecipitation', 'Sunrise', 'Sunset'\n",
    "\n",
    "]\n",
    "\n",
    "        # Filter the dataset to keep only the useful columns\n",
    "        filtered_data = df[useful_columns]\n",
    "\n",
    "        # Drop columns that are entirely NaN\n",
    "        cleaned_data = filtered_data.dropna(axis=1, how='all')\n",
    "        print(f\"Data cleaned successfully. Retained columns: {cleaned_data.shape[1]}\")\n",
    "\n",
    "        # Return the cleaned DataFrame\n",
    "        return cleaned_data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {csv_file_path}\")\n",
    "        return None\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing columns in data: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or cleaning CSV file {csv_file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CSV: C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\\2024_weather.csv\n",
      "Data cleaned successfully. Retained columns: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "      <th>DailySnowfall</th>\n",
       "      <th>DailyAverageWindSpeed</th>\n",
       "      <th>DailyPrecipitation</th>\n",
       "      <th>Sunrise</th>\n",
       "      <th>Sunset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01T00:51:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01T01:51:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01T02:51:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01T03:51:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01T04:51:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DATE HourlyPrecipitation  HourlyWindSpeed DailySnowfall  \\\n",
       "0  2024-01-01T00:51:00                0.00              7.0           NaN   \n",
       "1  2024-01-01T01:51:00                0.00              5.0           NaN   \n",
       "2  2024-01-01T02:51:00                0.00              0.0           NaN   \n",
       "3  2024-01-01T03:51:00                0.00              3.0           NaN   \n",
       "4  2024-01-01T04:51:00                0.00              0.0           NaN   \n",
       "\n",
       "   DailyAverageWindSpeed DailyPrecipitation  Sunrise  Sunset  \n",
       "0                    NaN                NaN      NaN     NaN  \n",
       "1                    NaN                NaN      NaN     NaN  \n",
       "2                    NaN                NaN      NaN     NaN  \n",
       "3                    NaN                NaN      NaN     NaN  \n",
       "4                    NaN                NaN      NaN     NaN  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\\2024_weather.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = load_and_clean_weather_data(csv_file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c9d6c-8a5e-4e45-8ea2-d5c525af923d",
   "metadata": {},
   "source": [
    "### Step 3: The following appears similar to functions above when processing taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_merge_weather_data(directory):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a directory using the load_and_clean_weather_data function,\n",
    "    and merge the resulting DataFrames into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing the CSV files.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A merged DataFrame containing cleaned data from all CSV files.\n",
    "    \"\"\"\n",
    "    all_data = []  # List to store cleaned DataFrames\n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "        # Only process files that are CSV\n",
    "        if os.path.isfile(file_path) and file_name.endswith('.csv'):\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            try:\n",
    "                # Apply the cleaning function to the CSV file\n",
    "                cleaned_data = load_and_clean_weather_data(file_path)\n",
    "                \n",
    "                # Append the cleaned DataFrame to the list if valid\n",
    "                if cleaned_data is not None and not cleaned_data.empty:\n",
    "                    all_data.append(cleaned_data)\n",
    "                else:\n",
    "                    print(f\"Skipping file {file_name}: No valid data found.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    if all_data:\n",
    "        merged_data = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"Successfully merged {len(all_data)} files into a single DataFrame.\")\n",
    "        return merged_data\n",
    "    else:\n",
    "        print(\"No valid data processed. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files were processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 2020_weather.csv\n",
      "Successfully loaded CSV: C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\\2020_weather.csv\n",
      "Data cleaned successfully. Retained columns: 8\n",
      "Processing file: 2021_weather.csv\n",
      "Successfully loaded CSV: C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\\2021_weather.csv\n",
      "Data cleaned successfully. Retained columns: 8\n",
      "Processing file: 2022_weather.csv\n",
      "Successfully loaded CSV: C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\\2022_weather.csv\n",
      "Data cleaned successfully. Retained columns: 8\n",
      "Processing file: 2023_weather.csv\n",
      "Successfully loaded CSV: C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\\2023_weather.csv\n",
      "Data cleaned successfully. Retained columns: 8\n",
      "Processing file: 2024_weather.csv\n",
      "Successfully loaded CSV: C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\\2024_weather.csv\n",
      "Data cleaned successfully. Retained columns: 8\n",
      "Successfully merged 5 files into a single DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Directory containing weather CSV files\n",
    "csv_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads weather\"\n",
    "\n",
    "# Process and merge all weather data\n",
    "merged_weather_data = process_and_merge_weather_data(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanWeather=merged_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "db1bd216-fd45-4b43-888d-33ae4aa55784",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanWeather.to_csv(\"CleanWeather_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "\n",
    "This section outlines the schema definitions and setup for the SQLite database used in the project. The database contains several tables designed to store weather data, taxi trip data, and Uber trip data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba72e22-14e0-4a5e-8649-0dcd1b44cd7a",
   "metadata": {},
   "source": [
    "### **Database Configuration**\n",
    "- **Database URL**: `sqlite:///project.db`\n",
    "- **Schema File**: `schema.sql` (optional file for saving schema definitions)\n",
    "- **Query Directory**: `queries` (optional directory for saving query files)\n",
    "\n",
    "A connection is established using:\n",
    "- **SQLite** via the `sqlite3` library.\n",
    "- **SQLAlchemy** for handling database operations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\"\n",
    "db_path = \"project.db\"\n",
    "engine = db.create_engine(DATABASE_URL)\n",
    "connection = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ad687-0a45-46e6-a248-5b2ede126bb2",
   "metadata": {},
   "source": [
    "#### **1. Hourly Weather Table**\n",
    "Stores hourly weather data such as precipitation and wind speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS HOURLY_WEATHER (\n",
    "   hourlyId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "   DATE TEXT,\n",
    "   HourlyPrecipitation REAL, \n",
    "   HourlyWindSpeed REAL\n",
    ");\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4edae-29db-4543-81a6-a4afe663b3a5",
   "metadata": {},
   "source": [
    "\n",
    "#### **2. Daily Weather Table**\n",
    "Stores daily weather data including snowfall, average wind speed, and precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dc18463a-c156-428c-94c5-3f691d00e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS DAILY_WEATHER (\n",
    "   dailyId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "   DATE TEXT,\n",
    "   DailySnowfall REAL, \n",
    "   DailyAverageWindSpeed REAL,\n",
    "   DailyPrecipitation REAL\n",
    ");\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93fa9da-be0a-4399-abec-07f6f9df3652",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **3. Taxi Trips Table**\n",
    "Stores taxi trip data including pickup time, trip distance, tips, total cost, congestion surcharge, dropoff_latitude and dropoff_longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "64be31af-55ac-43bc-9d25-be8f81ab3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS TAXI (\n",
    "   taxiId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "   tpep_pickup_datetime TEXT,\n",
    "   trip_distance REAL, \n",
    "   tip_amount REAL,\n",
    "   total_cost REAL,\n",
    "   congestion_surcharge REAL,\n",
    "   dropoff_latitude REAL,\n",
    "   dropoff_longitude REAL\n",
    ");\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e7a83-5c0d-4e98-bee9-bd02766ce159",
   "metadata": {},
   "source": [
    "#### **4. Uber Trips Table**\n",
    "Stores Uber trip data including pickup time, trip distance, tips, total cost, congestion surcharge, sales tax, toll charges, dropoff_latitude, dropoff_longitude and base_passenger_fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6d8b7ef3-5674-444c-8164-dc82c57a7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS UBER (\n",
    "   uberId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "   pickup_datetime TEXT,\n",
    "   trip_miles REAL, \n",
    "   tips REAL,\n",
    "   total_cost REAL,\n",
    "   congestion_surcharge REAL,\n",
    "   sales_tax REAL,\n",
    "   tolls REAL,\n",
    "   dropoff_latitude REAL,\n",
    "   dropoff_longitude REAL,\n",
    "   base_passenger_fare REAL\n",
    ");\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ffc50-a4dd-47c4-87b4-25537babaaa2",
   "metadata": {},
   "source": [
    "### Create `schema.sql` File\n",
    "\n",
    "This section demonstrates how to save the database schema into a file named `schema.sql`. This file contains the SQL commands required to create all the necessary tables for the project. By storing the schema in a separate file, you can version control the database structure and easily recreate the tables when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dc58a48d-a766-44a5-9787-2c04171c093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist table: ['HOURLY_WEATHER', 'sqlite_sequence', 'DAILY_WEATHER', 'TAXI', 'UBER', 'SUN']\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT name FROM sqlite_master WHERE type='table';\"))\n",
    "    tables = [row[0] for row in result.fetchall()]\n",
    "    print(\"Exist table:\", tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Clean and Insert Hourly Weather Data into Database\n",
    "\n",
    "This section describes how to clean the weather data and insert it into the `HOURLY_WEATHER` table in the database.\n",
    "\n",
    "#### **1. Load Weather Data**\n",
    "- The weather data is loaded from a CSV file using Pandas.\n",
    "- Example CSV file path:\n",
    "#### **2. Clean the Data**\n",
    "- **`HourlyPrecipitation`**:\n",
    "- Replaces invalid values like `'T'` and empty strings `''` with `0`.\n",
    "- Fills `NaN` values with `0`.\n",
    "- **`HourlyWindSpeed`**:\n",
    "- Replaces empty strings `''` with `0`.\n",
    "- Fills `NaN` values with `0`.\n",
    "\n",
    "#### **3. Extract Relevant Columns**\n",
    "- Columns `DATE`, `HourlyPrecipitation`, and `HourlyWindSpeed` are selected for insertion into the `HOURLY_WEATHER` table.\n",
    "\n",
    "#### **4. Filter the 'DATE' column ends with '23:59:00'**\n",
    "\n",
    "#### **5. Insert Data into the Database**\n",
    "- The cleaned data is inserted into the `HOURLY_WEATHER` table using the Pandas `to_sql()` method.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  DATE HourlyPrecipitation  HourlyWindSpeed DailySnowfall  \\\n",
      "0  2020-01-01T00:51:00                0.00              8.0           NaN   \n",
      "1  2020-01-01T01:51:00                0.00              8.0           NaN   \n",
      "2  2020-01-01T02:51:00                0.00             14.0           NaN   \n",
      "3  2020-01-01T03:51:00                0.00             11.0           NaN   \n",
      "4  2020-01-01T04:51:00                0.00              6.0           NaN   \n",
      "\n",
      "   DailyAverageWindSpeed DailyPrecipitation  Sunrise  Sunset  \n",
      "0                    NaN                NaN      NaN     NaN  \n",
      "1                    NaN                NaN      NaN     NaN  \n",
      "2                    NaN                NaN      NaN     NaN  \n",
      "3                    NaN                NaN      NaN     NaN  \n",
      "4                    NaN                NaN      NaN     NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56098"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the empty data and filled with  which can help us insert it easier\n",
    "weather_data = pd.read_csv(r\"C:\\Users\\wenji\\OneDrive\\Project\\CleanWeather_data.csv\")\n",
    "weather_data['HourlyPrecipitation'] = weather_data['HourlyPrecipitation'].replace(['T', ''], 0).fillna(0)\n",
    "weather_data['HourlyWindSpeed'] = weather_data['HourlyWindSpeed'].replace([''], 0).fillna(0)\n",
    "print(weather_data.head())\n",
    "# Insert the hourly data in to table: HOURLY_WEATHER\n",
    "hourly_columns = ['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']\n",
    "hourly_weather_data = weather_data[hourly_columns]\n",
    "hourly_weather_data.to_sql('HOURLY_WEATHER', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "903cccdd-722f-4791-92e5-ce60f19f0a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '2020-01-01T00:51:00', 0.0, 8.0)\n",
      "(2, '2020-01-01T01:51:00', 0.0, 8.0)\n",
      "(3, '2020-01-01T02:51:00', 0.0, 14.0)\n",
      "(4, '2020-01-01T03:51:00', 0.0, 11.0)\n",
      "(5, '2020-01-01T04:51:00', 0.0, 6.0)\n"
     ]
    }
   ],
   "source": [
    "# Check whether the insert data correct\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT * FROM HOURLY_WEATHER LIMIT 5;\"))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9928bc82-4af0-4a56-9227-af711bbd687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    DATE HourlyPrecipitation  HourlyWindSpeed DailySnowfall  \\\n",
      "24   2020-01-01T23:59:00                   0              0.0           0.0   \n",
      "49   2020-01-02T23:59:00                   0              0.0           0.0   \n",
      "86   2020-01-03T23:59:00                   0              0.0           0.0   \n",
      "144  2020-01-04T23:59:00                   0              0.0           0.0   \n",
      "169  2020-01-05T23:59:00                   0              0.0           0.0   \n",
      "205  2020-01-06T23:59:00                   0              0.0           0.2   \n",
      "232  2020-01-07T23:59:00                   0              0.0           0.0   \n",
      "\n",
      "     DailyAverageWindSpeed DailyPrecipitation  Sunrise  Sunset  \n",
      "24                     8.6               0.00    720.0  1639.0  \n",
      "49                     5.4               0.00    720.0  1640.0  \n",
      "86                     3.4               0.15    720.0  1641.0  \n",
      "144                    4.4               0.27    720.0  1642.0  \n",
      "169                   11.3                  0    720.0  1643.0  \n",
      "205                    6.1               0.04    720.0  1644.0  \n",
      "232                    5.1                  0    720.0  1644.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the 'DATE' column ends with '23:59:00'\n",
    "daily_data = weather_data[weather_data['DATE'].str.endswith('23:59:00')].copy()\n",
    "# Replace missing values in column\n",
    "daily_data['DailySnowfall'] = daily_data['DailySnowfall'].replace(['T', '', None], 0).fillna(0)\n",
    "daily_data['DailyAverageWindSpeed'] = weather_data['DailyAverageWindSpeed'].replace(['T', '', None], 0).fillna(0)\n",
    "daily_data['DailyPrecipitation'] = weather_data['DailyPrecipitation'].replace(['T', '', None], 0).fillna(0)\n",
    "print(daily_data.head(7))\n",
    "# Insert the data into table: DAILY_WEATHER\n",
    "daily_columns = ['DATE', 'DailySnowfall', 'DailyAverageWindSpeed', 'DailyPrecipitation']\n",
    "daily_weather_data = daily_data[daily_columns]\n",
    "daily_weather_data.to_sql('DAILY_WEATHER', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9138802d-ac12-4842-8291-273a7039b955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2020-01-01T23:59:00', '0.0', 8.6, '0.00')\n",
      "('2020-01-02T23:59:00', '0.0', 5.4, '0.00')\n",
      "('2020-01-03T23:59:00', '0.0', 3.4, '0.15')\n",
      "('2020-01-04T23:59:00', '0.0', 4.4, '0.27')\n",
      "('2020-01-05T23:59:00', '0.0', 11.3, '0')\n",
      "('2020-01-06T23:59:00', '0.2', 6.1, '0.04')\n",
      "('2020-01-07T23:59:00', '0.0', 5.1, '0')\n"
     ]
    }
   ],
   "source": [
    "# Check whether the insert data correct\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT * FROM DAILY_WEATHER LIMIT 7 ;\"))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddacdbae-b8c3-4e0a-b364-900603b2ef54",
   "metadata": {},
   "source": [
    "### Clean and Insert Taxi Data into Database\n",
    "\n",
    "This section explains how to calculate the total cost for taxi trips and insert the processed data into the `TAXI` table in the database.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "#### **1. Load Taxi Data**\n",
    "- Load taxi trip data from a CSV file using Pandas.\n",
    "- Example CSV file path:\n",
    "#### **2. Calculate Total Cost**\n",
    "- Add `total_amount` and `congestion_surcharge` columns to compute the total trip cost:\n",
    "#### **3. Extract Relevant Columns**\n",
    "- Select the columns for insertion into the database:\n",
    "- `tpep_pickup_datetime`, `trip_distance`, `tip_amount`, `total_cost`, and `congestion_surcharge`.\n",
    "\n",
    "#### **4. Insert Data into Database**\n",
    "- Insert the cleaned taxi trip data into the `TAXI` table using Pandas `to_sql()`. The table is replaced if it already exists (`if_exists='replace'`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ca2876c3-9ec6-404b-8133-7235223dd6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2020-01-25 10:49:58   2020-01-25 11:07:35              1.0           3.28   \n",
      "1  2020-01-15 07:30:08   2020-01-15 07:40:01              1.0           1.75   \n",
      "2  2020-01-09 06:29:09   2020-01-09 06:35:44              1.0           0.87   \n",
      "3  2020-01-26 12:24:04   2020-01-26 12:29:15              2.0           0.98   \n",
      "4  2020-01-30 07:57:53   2020-01-30 08:10:19              1.0           1.30   \n",
      "\n",
      "   pulocationid  dolocationid  tip_amount  total_amount  congestion_surcharge  \\\n",
      "0         142.0         246.0        1.70         19.00                   2.5   \n",
      "1         238.0         166.0        1.20         13.00                   2.5   \n",
      "2         100.0         164.0        0.00          8.80                   2.5   \n",
      "3         161.0          43.0        0.00          8.80                   2.5   \n",
      "4         229.0         262.0        2.45         14.75                   2.5   \n",
      "\n",
      "   pickup_latitude  pickup_longitude  dropoff_latitude  dropoff_longitude  \\\n",
      "0        40.791705        -73.973049         40.809457         -73.961764   \n",
      "1        40.753513        -73.988787         40.748575         -73.985156   \n",
      "2        40.758028        -73.977698         40.782478         -73.965554   \n",
      "3        40.756729        -73.965146         40.775932         -73.946510   \n",
      "4        40.756688        -73.972356         40.758028         -73.977698   \n",
      "\n",
      "   total_cost  \n",
      "0       21.50  \n",
      "1       15.50  \n",
      "2       11.30  \n",
      "3       11.30  \n",
      "4       17.25  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21182"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding total amount and congestion_surcharge we got the final cost\n",
    "taxi_data = pd.read_csv(r\"C:\\Users\\wenji\\OneDrive\\Project\\taxi_data.csv\")\n",
    "taxi_data['total_cost'] = taxi_data['total_amount'] + taxi_data['congestion_surcharge']\n",
    "print(taxi_data.head())\n",
    "# Insert the data into table: TAXI\n",
    "taxi_columns = ['tpep_pickup_datetime', 'trip_distance', 'tip_amount', 'total_cost', 'congestion_surcharge', 'dropoff_latitude', 'dropoff_longitude']\n",
    "taxi_table_data = taxi_data[taxi_columns]\n",
    "taxi_table_data.to_sql('TAXI', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2ac2e31d-e3b4-4823-a878-402c601aed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2020-01-25 10:49:58', 3.28, 1.7, 21.5, 2.5, 40.809456961125264, -73.96176359682917)\n",
      "('2020-01-15 07:30:08', 1.75, 1.2, 15.5, 2.5, 40.74857462935672, -73.98515639467682)\n",
      "('2020-01-09 06:29:09', 0.87, 0.0, 11.3, 2.5, 40.78247809974789, -73.96555356545916)\n",
      "('2020-01-26 12:24:04', 0.98, 0.0, 11.3, 2.5, 40.77593240314994, -73.94651035601464)\n",
      "('2020-01-30 07:57:53', 1.3, 2.45, 17.25, 2.5, 40.75802804352625, -73.97769793122406)\n"
     ]
    }
   ],
   "source": [
    "# Check whether the insert data correct\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT * FROM TAXI LIMIT 5 ;\"))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32cfd89-817b-4723-8bc6-26e5c001ae7f",
   "metadata": {},
   "source": [
    "### Clean and Insert Uber Data into Database\n",
    "\n",
    "This section explains how to clean Uber trip data, calculate the total cost of trips, and insert the processed data into the `UBER` table in the database.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "#### **1. Load Uber Data**\n",
    "- Load Uber trip data from a CSV file using Pandas.\n",
    "- Example CSV file path:\n",
    "#### **2. Clean Data**\n",
    "- Drop rows where `pickup_datetime` is missing to ensure all rows have valid timestamps.\n",
    "- Calculate the total trip cost using the formula:\n",
    "\n",
    "#### **3. Extract Relevant Columns**\n",
    "- Select the columns for insertion into the database:\n",
    "- `pickup_datetime`, `trip_miles`, `tips`, `total_cost`, `tolls`, `sales_tax`, and `congestion_surcharge`.\n",
    "\n",
    "#### **4. Insert Data into Database**\n",
    "- Insert the cleaned Uber trip data into the `UBER` table using Pandas `to_sql()`. The table is replaced if it already exists (`if_exists='replace'`).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "41f0fe64-a78c-48f9-a097-e3614210fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pickup_datetime     dropoff_datetime  pulocationid  dolocationid  \\\n",
      "0  2020-01-06 17:55:52  2020-01-06 18:58:16          48.0         132.0   \n",
      "1  2020-01-17 13:34:41  2020-01-17 13:45:42         263.0          43.0   \n",
      "2  2020-01-20 13:09:30  2020-01-20 13:28:12         188.0          63.0   \n",
      "3  2020-01-11 12:56:38  2020-01-11 13:06:01         143.0          50.0   \n",
      "4  2020-01-28 00:04:23  2020-01-28 00:20:18         162.0         160.0   \n",
      "\n",
      "   trip_miles  trip_time  base_passenger_fare  tolls   bcf  sales_tax  \\\n",
      "0      20.001     3744.0                62.90   5.41  1.71       6.06   \n",
      "1       1.290      661.0                 9.74   0.00  0.00       0.00   \n",
      "2       3.350     1122.0                 7.47   0.00  0.19       0.66   \n",
      "3       1.363      563.0                 8.11   0.00  0.20       0.70   \n",
      "4       6.680      955.0                 5.18   3.06  0.22       0.76   \n",
      "\n",
      "   congestion_surcharge  airport_fee  tips  pickup_latitude  pickup_longitude  \\\n",
      "0                  2.75          NaN   0.0        40.762253        -73.989845   \n",
      "1                  2.75          NaN   0.0        40.778766        -73.951010   \n",
      "2                  0.00          NaN   0.0        40.658744        -73.947442   \n",
      "3                  2.75          NaN   2.0        40.775965        -73.987646   \n",
      "4                  0.75          NaN   0.0        40.756688        -73.972356   \n",
      "\n",
      "   dropoff_latitude  dropoff_longitude  total_cost  \n",
      "0         40.646985         -73.786533       77.12  \n",
      "1         40.782478         -73.965554       12.49  \n",
      "2         40.683840         -73.878173        8.13  \n",
      "3         40.766238         -73.995135       13.56  \n",
      "4         40.718337         -73.880051        9.75  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19903"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NA and calculate the total_cost\n",
    "uber = pd.read_csv(r\"C:\\Users\\wenji\\OneDrive\\Project\\Uber_data.csv\")\n",
    "uber_data = uber.dropna(subset=['pickup_datetime']).copy()\n",
    "uber_data['total_cost'] = uber_data['base_passenger_fare'] + uber_data['tolls']  + uber_data['sales_tax'] + uber_data['congestion_surcharge']  + uber_data['tips']\n",
    "print(uber_data.head())\n",
    "# Insert the data into table: TAXI\n",
    "uber_columns = ['pickup_datetime', 'trip_miles', 'tips', 'total_cost','tolls','sales_tax','congestion_surcharge', 'dropoff_latitude', 'dropoff_longitude', 'base_passenger_fare']\n",
    "uber_table_data = uber_data[uber_columns]\n",
    "uber_table_data.to_sql('UBER', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "91e2f505-c96d-4700-9a02-219c605710f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2020-01-06 17:55:52', 20.001, 0.0, 77.12, 5.41, 6.06, 2.75, 40.64698489239515, -73.78653298334973, 62.9)\n",
      "('2020-01-17 13:34:41', 1.29, 0.0, 12.49, 0.0, 0.0, 2.75, 40.78247809974789, -73.96555356545916, 9.74)\n",
      "('2020-01-20 13:09:30', 3.35, 0.0, 8.129999999999999, 0.0, 0.66, 0.0, 40.68383976195103, -73.87817261050249, 7.47)\n",
      "('2020-01-11 12:56:38', 1.363, 2.0, 13.559999999999999, 0.0, 0.7, 2.75, 40.76623772504186, -73.99513522075517, 8.11)\n",
      "('2020-01-28 00:04:23', 6.68, 0.0, 9.75, 3.06, 0.76, 0.75, 40.71833677001376, -73.88005140814958, 5.18)\n"
     ]
    }
   ],
   "source": [
    "# Check whether the insert data correct\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT * FROM UBER LIMIT 5 ;\"))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7eb66-f358-4ac9-8532-d5cae25bc5c7",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as file:\n",
    "        file.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1 to Identify the Most Popular Hour to Take a Taxi\n",
    "\n",
    "This section describes how to determine the most popular hour of the day for taking a taxi, based on the number of taxi trips.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "#### **1. SQL Query Definition**\n",
    "- The query retrieves data from the `TAXI` table.\n",
    "- **Query Details**:\n",
    "  - Extracts the hour (`%H`) from the `tpep_pickup_datetime` column using `strftime()`.\n",
    "  - Groups trips by hour and counts the number of trips (`COUNT(*)`) for each hour.\n",
    "  - Orders the results in descending order of trip counts (`ORDER BY taxi_num DESC`).\n",
    "\n",
    "#### **2. Execute SQL Query**\n",
    "- The query is executed using Pandas' `read_sql()` method to retrieve the hourly trip counts.\n",
    "\n",
    "#### **3. Analyze the Results**\n",
    "- The first row of the result represents the hour with the highest number of taxi trips.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"most_popular_hour_to_take_a_taxi.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT strftime('%H', tpep_pickup_datetime) AS Hour, COUNT(*) AS taxi_num\n",
    "FROM TAXI\n",
    "WHERE tpep_pickup_datetime BETWEEN '2020-01-01 00:00:00' AND '2024-08-31 23:59:59'\n",
    "GROUP BY Hour\n",
    "ORDER BY taxi_num DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>taxi_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>1363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19</td>\n",
       "      <td>1219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>1059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>1051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>09</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>08</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>07</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>06</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>01</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>02</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>03</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>05</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>04</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hour  taxi_num\n",
       "0    18      1503\n",
       "1    17      1450\n",
       "2    15      1405\n",
       "3    14      1363\n",
       "4    16      1346\n",
       "5    12      1245\n",
       "6    13      1239\n",
       "7    19      1219\n",
       "8    11      1059\n",
       "9    20      1051\n",
       "10   21      1044\n",
       "11   10      1034\n",
       "12   09       943\n",
       "13   22       879\n",
       "14   08       827\n",
       "15   23       737\n",
       "16   07       602\n",
       "17   00       508\n",
       "18   06       369\n",
       "19   01       365\n",
       "20   02       211\n",
       "21   03       169\n",
       "22   05       152\n",
       "23   04       106"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61fac8-634a-4b8a-a7e3-a77cd60dcf5f",
   "metadata": {},
   "source": [
    "### Query 2 to Identify the Most Popular Day of the Week to Take an Uber\n",
    "\n",
    "This section describes how to determine the most popular day of the week for Uber rides, based on the number of trips.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "#### **1. SQL Query Definition**\n",
    "- The query retrieves data from the `UBER` table.\n",
    "- **Query Details**:\n",
    "  - Extracts the day of the week (`%w`) from the `pickup_datetime` column using `strftime()`.\n",
    "    - `0` = Sunday, `1` = Monday, ..., `6` = Saturday.\n",
    "  - Groups trips by day and counts the number of trips (`COUNT(*)`) for each day.\n",
    "  - Orders the results in descending order of trip counts (`ORDER BY Day_num DESC`).\n",
    "\n",
    "#### **2. Execute SQL Query**\n",
    "- The query is executed using Pandas' `read_sql()` method to retrieve the trip counts for each day of the week.\n",
    "\n",
    "#### **3. Analyze the Results**\n",
    "- The first row of the result represents the day with the highest number of Uber trips.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e3e97572-c4d7-40aa-9696-337ee70e06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = \"most_popular_day_of_the_week_to_take_an_Uber.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT strftime('%w', pickup_datetime) AS Day, COUNT(*) AS Day_num\n",
    "FROM UBER\n",
    "GROUP BY Day\n",
    "ORDER BY Day_num DESC;\n",
    "\"\"\"\n",
    "# I use the strftime('%w', pickup_datetime) to take the day of the week.\n",
    "# So 0 = Sunday, 1 = Monday, 2 = Tuesday, 3 = Wednesday, 4 = Thursday, 5 = Friday, 6 = Saturday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1faa8b3f-c3dd-4711-90ea-db9a40b8159a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Day_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>3361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Day  Day_num\n",
       "0   6     3361\n",
       "1   5     3142\n",
       "2   4     2854\n",
       "3   0     2817\n",
       "4   3     2721\n",
       "5   2     2614\n",
       "6   1     2394"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(QUERY_2, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "97bde04a-a77e-4bdc-bb8c-e6209155e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52101c3-8333-4684-8e34-fdc6a8eb559d",
   "metadata": {},
   "source": [
    "### Query 3 to Calculate the 95th Percentile of Trip Distance in January 2024\n",
    "\n",
    "This section describes how to compute the 95th percentile of trip distances for both taxi and Uber trips during January 2024.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "#### **1. SQL Query Definition**\n",
    "- The SQL query combines data from the `TAXI` and `UBER` tables.\n",
    "- **Query Details**:\n",
    "  - For the `TAXI` table, it selects `trip_distance` for trips with pickup times between `'2024-01-01 00:00:00'` and `'2024-01-31 23:59:59'`.\n",
    "  - For the `UBER` table, it selects `trip_miles` within the same time range.\n",
    "  - Combines both datasets using a `UNION ALL` to preserve all records.\n",
    "\n",
    "#### **2. Execute SQL Query**\n",
    "- The query is executed using Pandas' `read_sql()` method to retrieve the combined dataset.\n",
    "\n",
    "#### **3. Calculate the 95th Percentile**\n",
    "- Use the Pandas `quantile()` method on the `distance` column to compute the 95th percentile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3c82f8da-b3cf-4a83-a43e-902ff4355608",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = \"95%_percentile_of_trip_distance_in_January_2024.sql\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "SELECT trip_distance AS distance\n",
    "FROM TAXI\n",
    "WHERE tpep_pickup_datetime BETWEEN '2024-01-01 00:00:00' AND '2024-01-31 23:59:59'\n",
    "UNION ALL\n",
    "SELECT trip_miles AS distance\n",
    "FROM UBER\n",
    "WHERE pickup_datetime BETWEEN '2024-01-01 00:00:00' AND '2024-01-31 23:59:59';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5096107b-5349-42cf-93f7-1abe8ef94c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.771499999999996"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_sql(QUERY_3, con=engine)\n",
    "percentile_95 = data['distance'].quantile(0.95)\n",
    "percentile_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "caeb653d-b113-454d-8f29-ed05ff858184",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c11a64b-e05e-41e0-aacc-8b1ac0911d12",
   "metadata": {},
   "source": [
    "### Query 4 to Analyze Weather for the Busiest Days in 2023\n",
    "\n",
    "This section describes how to determine the weather conditions for the busiest days in 2023 based on hired ride data (Taxi and Uber).\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "#### **1. SQL Query Definition**\n",
    "The query combines ride and weather data to identify the top 10 busiest days in 2023 and their corresponding weather conditions.\n",
    "\n",
    "- **Subqueries**:\n",
    "  1. **`HiredRides`**:\n",
    "     - Combines ride data from both the `TAXI` and `UBER` tables.\n",
    "     - Extracts the date and trip distances.\n",
    "  2. **`Rides`**:\n",
    "     - Groups rides by date.\n",
    "     - Calculates:\n",
    "       - Total number of rides (`hired_num`).\n",
    "       - Average trip distance (`average_distance`).\n",
    "  3. **`Weather`**:\n",
    "     - Extracts daily weather data from the `DAILY_WEATHER` table, including:\n",
    "       - Daily precipitation (`DailyPrecipitation`).\n",
    "       - Average wind speed (`DailyAverageWindSpeed`).\n",
    "     - Groups weather data by date.\n",
    "\n",
    "- **Main Query**:\n",
    "  - Joins `Rides` with `Weather` on the date.\n",
    "  - Selects the top 10 dates with the highest number of rides, along with weather conditions for those days.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6d220aeb-6423-43ff-89ba-060403090a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = \"weather_like_for_the_busiest_days_in_2023.sql\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "WITH HiredRides AS (\n",
    "    SELECT DATE(tpep_pickup_datetime) AS date, trip_distance AS distance\n",
    "    FROM TAXI\n",
    "    WHERE tpep_pickup_datetime BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    UNION ALL\n",
    "    SELECT DATE(pickup_datetime) AS date,trip_miles AS distance\n",
    "    FROM UBER\n",
    "    WHERE pickup_datetime BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    ),\n",
    "Rides AS (\n",
    "    SELECT date, COUNT(*) AS hired_num, AVG(distance) AS average_distance\n",
    "    FROM HiredRides\n",
    "    GROUP BY date\n",
    "    ),\n",
    "Weather AS (\n",
    "    SELECT DATE(DATE) AS weather_date, DailyPrecipitation AS average_precipitation_amount, DailyAverageWindSpeed AS average_wind_speed\n",
    "    FROM DAILY_WEATHER\n",
    "    WHERE DATE BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    GROUP BY weather_date\n",
    "    )\n",
    "SELECT \n",
    "    Rides.date AS date,\n",
    "    Rides.hired_num AS total_rides,\n",
    "    Rides.average_distance AS average_distance,\n",
    "    Weather.average_precipitation_amount average_precipitation_amount,\n",
    "    Weather.average_wind_speed AS average_wind_speed\n",
    "FROM Rides\n",
    "LEFT JOIN Weather ON Rides.date = Weather.weather_date\n",
    "ORDER BY Rides.hired_num DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0664f735-5131-42ba-ac16-fb87601a2c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>total_rides</th>\n",
       "      <th>average_distance</th>\n",
       "      <th>average_precipitation_amount</th>\n",
       "      <th>average_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-09</td>\n",
       "      <td>41</td>\n",
       "      <td>3.065024</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>38</td>\n",
       "      <td>4.515526</td>\n",
       "      <td>0</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>37</td>\n",
       "      <td>4.405432</td>\n",
       "      <td>2.32</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>36</td>\n",
       "      <td>4.502111</td>\n",
       "      <td>0.36</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-20</td>\n",
       "      <td>36</td>\n",
       "      <td>3.483917</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-09-14</td>\n",
       "      <td>36</td>\n",
       "      <td>3.821806</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-05-05</td>\n",
       "      <td>35</td>\n",
       "      <td>3.476971</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-07-13</td>\n",
       "      <td>35</td>\n",
       "      <td>3.646886</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-09-16</td>\n",
       "      <td>35</td>\n",
       "      <td>4.580657</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>35</td>\n",
       "      <td>3.478343</td>\n",
       "      <td>0.81</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  total_rides  average_distance average_precipitation_amount  \\\n",
       "0  2023-11-09           41          3.065024                         0.00   \n",
       "1  2023-07-28           38          4.515526                            0   \n",
       "2  2023-04-30           37          4.405432                         2.32   \n",
       "3  2023-03-10           36          4.502111                         0.36   \n",
       "4  2023-05-20           36          3.483917                         0.77   \n",
       "5  2023-09-14           36          3.821806                         0.00   \n",
       "6  2023-05-05           35          3.476971                         0.00   \n",
       "7  2023-07-13           35          3.646886                         0.00   \n",
       "8  2023-09-16           35          4.580657                         0.00   \n",
       "9  2023-10-14           35          3.478343                         0.81   \n",
       "\n",
       "   average_wind_speed  \n",
       "0                 3.2  \n",
       "1                 2.4  \n",
       "2                 6.9  \n",
       "3                 3.9  \n",
       "4                 2.8  \n",
       "5                 5.4  \n",
       "6                 1.9  \n",
       "7                 2.4  \n",
       "8                 6.3  \n",
       "9                 4.4  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(QUERY_4, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1254409e-c87b-4580-b7a3-fee514b4d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c00b69-8c5f-461e-8961-50172f7f18d6",
   "metadata": {},
   "source": [
    "### Query 5 to Analyze Rides During Snow Days\n",
    "\n",
    "This section describes how to determine the number of rides hired during snow days, focusing on the days with the highest snowfall.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "#### **1. SQL Query Definition**\n",
    "The query combines weather and ride data to identify the top 10 snowiest days and the corresponding number of rides on those days.\n",
    "\n",
    "- **Subqueries**:\n",
    "  1. **`SnowDay`**:\n",
    "     - Extracts dates with snowfall (`DailySnowfall > 0`) from the `DAILY_WEATHER` table.\n",
    "     - Calculates total snowfall for each snow day.\n",
    "  2. **`HiredRides`**:\n",
    "     - Combines ride data from both `TAXI` and `UBER` tables.\n",
    "     - Groups rides by date and counts the total number of rides for each date.\n",
    "\n",
    "- **Main Query**:\n",
    "  - Joins `SnowDay` with `HiredRides` on the date.\n",
    "  - Selects:\n",
    "    - The snow day (`snow_day`).\n",
    "    - Total snowfall (`total_snowfall_of_snow_day`).\n",
    "    - Total number of rides (`total_rides`) on that day.\n",
    "  - Orders the results by snowfall in descending order and limits to the top 10 days.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "69d25360-ae4a-4f76-812f-87464c376418",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = \"rides_were_hired_during_snow_days.sql\"\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "WITH SnowDay AS (\n",
    "    SELECT DATE(DATE) AS snow_day, DailySnowfall AS total_snowfall_of_snow_day\n",
    "    FROM DAILY_WEATHER\n",
    "    WHERE DailySnowfall > 0\n",
    "    GROUP BY snow_day\n",
    "    ),\n",
    "HiredRides AS (\n",
    "    SELECT DATE(tpep_pickup_datetime) AS date, COUNT(*) AS total_rides\n",
    "    FROM TAXI\n",
    "    GROUP BY date\n",
    "    UNION ALL\n",
    "    SELECT DATE(pickup_datetime) AS date, COUNT(*) AS total_rides\n",
    "    FROM UBER\n",
    "    GROUP BY date\n",
    "    )\n",
    "SELECT \n",
    "    SnowDay.snow_day AS date,\n",
    "    SnowDay.total_snowfall_of_snow_day,\n",
    "    HiredRides.total_rides\n",
    "FROM SnowDay\n",
    "JOIN HiredRides ON SnowDay.snow_day = HiredRides.date\n",
    "ORDER BY SnowDay.total_snowfall_of_snow_day DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "29e69da0-41be-4a64-b50e-88f3179c7955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>total_snowfall_of_snow_day</th>\n",
       "      <th>total_rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-29</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-29</td>\n",
       "      <td>7.3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>6.5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>6.5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>5.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>5.8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-12-17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-12-17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date total_snowfall_of_snow_day  total_rides\n",
       "0  2022-01-29                        7.3            4\n",
       "1  2022-01-29                        7.3           13\n",
       "2  2020-12-16                        6.5           10\n",
       "3  2020-12-16                        6.5           13\n",
       "4  2022-01-07                        5.8            7\n",
       "5  2022-01-07                        5.8           10\n",
       "6  2021-02-07                        4.5            6\n",
       "7  2021-02-07                        4.5            9\n",
       "8  2020-12-17                        4.0            2\n",
       "9  2020-12-17                        4.0            8"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(QUERY_5, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "598a88f5-80cb-4384-97af-429c4df97576",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c973a0-8891-4238-bee3-de54fd47d406",
   "metadata": {},
   "source": [
    "### Query 6 to Analyze Rides and Weather Hourly from September 25 to October 3, 2023\n",
    "\n",
    "This section describes how to analyze the number of rides hired (Taxi and Uber) and the corresponding weather conditions (precipitation and wind speed) for each hour during the period from September 25 to October 3, 2023.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "#### **1. SQL Query Definition**\n",
    "The query combines ride data and hourly weather data to analyze the relationship between weather conditions and ride demand on an hourly basis.\n",
    "\n",
    "- **Subqueries**:\n",
    "  1. **`HiredRiders`**:\n",
    "     - Extracts hourly ride data from both `TAXI` and `UBER` tables.\n",
    "     - Groups rides by `strftime('%Y-%m-%d %H', datetime)` to get the total number of rides for each hour.\n",
    "  2. **`Weather`**:\n",
    "     - Extracts hourly weather data from the `HOURLY_WEATHER` table, including:\n",
    "       - Total precipitation (`HourlyPrecipitation`).\n",
    "       - Average wind speed (`HourlyWindSpeed`).\n",
    "     - Groups weather data by the same hourly format.\n",
    "\n",
    "- **Main Query**:\n",
    "  - Joins `HiredRiders` and `Weather` on the hourly date and time.\n",
    "  - Retrieves the number of rides, total precipitation, and average wind speed for each hour.\n",
    "  - Orders the results chronologically by hour.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8e41e955-671f-446d-806f-42b6b4d62047",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6_FILENAME = \"rides_were_hired_during_snow_days.sql\"\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "WITH HiredRiders AS (\n",
    "    SELECT strftime('%Y-%m-%d %H', tpep_pickup_datetime) AS date_hour, COUNT(*) AS hired_num\n",
    "    FROM TAXI\n",
    "    WHERE tpep_pickup_datetime BETWEEN '2023-09-25 00:00:00' AND '2023-10-03 23:59:59'\n",
    "    GROUP BY date_hour\n",
    "    UNION ALL\n",
    "    SELECT strftime('%Y-%m-%d %H', pickup_datetime) AS date_hour, COUNT(*) AS hired_num\n",
    "    FROM UBER\n",
    "    WHERE pickup_datetime BETWEEN '2023-09-25 00:00:00' AND '2023-10-03 23:59:59'\n",
    "    GROUP BY date_hour\n",
    "    ),\n",
    "Weather AS (\n",
    "    SELECT \n",
    "        strftime('%Y-%m-%d %H', DATE) AS weather_date_hour,\n",
    "        HourlyPrecipitation AS total_precipitation_for_the_hour,\n",
    "        HourlyWindSpeed AS average_wind_speed_for_the_hour\n",
    "    FROM HOURLY_WEATHER\n",
    "    WHERE Date BETWEEN '2023-09-25 00:00:00' AND '2023-10-03 23:59:59'\n",
    "    GROUP BY weather_date_hour\n",
    "    )\n",
    "SELECT \n",
    "    HiredRiders.date_hour AS date_hour,\n",
    "    HiredRiders.hired_num,\n",
    "    Weather.total_precipitation_for_the_hour,\n",
    "    Weather.average_wind_speed_for_the_hour\n",
    "FROM HiredRiders\n",
    "LEFT JOIN Weather ON HiredRiders.date_hour = Weather.weather_date_hour\n",
    "ORDER BY HiredRiders.date_hour;\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
