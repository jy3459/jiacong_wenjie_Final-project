{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "Done by Group7 : Jiacong Yuan ( jy3459 ) and Wenjie Lin ( wl2792 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from functools import partial\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones and Lookup Coordinates\n",
    "This section demonstrates how to load a geospatial dataset representing NYC taxi zones and find the geographic coordinates (latitude and longitude) for a specific taxi zone using its unique Location ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882de2ce-006b-416f-89fd-09662c05b5ec",
   "metadata": {},
   "source": [
    "### Step 1: Load the Taxi Zones Data\n",
    "\n",
    "#### **What Happens in This Step**\n",
    "\n",
    "1. **Input File Path**  \n",
    "   - The file path for the shapefile is specified. It should contain a map of NYC taxi zones, each associated with a unique `LocationID`.\n",
    "\n",
    "2. **Check for File Existence**  \n",
    "   - The program first verifies if the shapefile exists at the specified path. If the file is missing, it raises an error and stops the execution.\n",
    "\n",
    "3. **Load the Shapefile**  \n",
    "   - The file is loaded into a GeoDataFrame using GeoPandas, which is specialized for working with spatial data.\n",
    "\n",
    "4. **Validate the Data**  \n",
    "   - It ensures that the shapefile contains the required column `LocationID` (a unique identifier for each taxi zone).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5772e96f-ccab-48fd-ae94-0ca34e24889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile_path):\n",
    "    \"\"\"\n",
    "    Load the Taxi Zone shapefile into a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        shapefile_path (str): Path to the Taxi Zone shapefile.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: A GeoDataFrame containing Taxi Zone data.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the shapefile does not exist.\n",
    "        RuntimeError: If there is an error loading the shapefile.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(shapefile_path):\n",
    "        raise FileNotFoundError(f\"Shapefile not found at: {shapefile_path}\")\n",
    "\n",
    "    try:\n",
    "        taxi_zones_gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        if 'LocationID' not in taxi_zones_gdf.columns:\n",
    "            raise ValueError(\"Shapefile must contain 'LocationID' column.\")\n",
    "\n",
    "        # Ensure CRS is EPSG:4326\n",
    "        if taxi_zones_gdf.crs is None or taxi_zones_gdf.crs.to_epsg() != 4326:\n",
    "            taxi_zones_gdf = taxi_zones_gdf.to_crs(epsg=4326)\n",
    "\n",
    "        return taxi_zones_gdf\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load shapefile: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi Zone Shapefile found at: C:\\Users\\wenji\\OneDrive\\Project\\taxi_zones/taxi_zones.shp\n"
     ]
    }
   ],
   "source": [
    "TAXI_ZONES_DIR = r\"C:\\Users\\wenji\\OneDrive\\Project\\taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "if not os.path.exists(TAXI_ZONES_SHAPEFILE):\n",
    "    raise FileNotFoundError(f\"Taxi Zone Shapefile not found at: {TAXI_ZONES_SHAPEFILE}\")\n",
    "else:\n",
    "    print(f\"Taxi Zone Shapefile found at: {TAXI_ZONES_SHAPEFILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ffaea61-39d0-4a72-9f01-da0663b8413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
      "0         1    0.116357    0.000782           Newark Airport           1   \n",
      "1         2    0.433470    0.004866              Jamaica Bay           2   \n",
      "2         3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
      "3         4    0.043567    0.000112            Alphabet City           4   \n",
      "4         5    0.092146    0.000498            Arden Heights           5   \n",
      "\n",
      "         borough                                           geometry  \n",
      "0            EWR  POLYGON ((-74.18445 40.695, -74.18449 40.6951,...  \n",
      "1         Queens  MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ...  \n",
      "2          Bronx  POLYGON ((-73.84793 40.87134, -73.84725 40.870...  \n",
      "3      Manhattan  POLYGON ((-73.97177 40.72582, -73.97179 40.725...  \n",
      "4  Staten Island  POLYGON ((-74.17422 40.56257, -74.17349 40.562...  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    taxi_zones_gdf = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "    print(taxi_zones_gdf.head())\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading shapefile: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e2f4e4-de25-4f22-8a8b-377d0fad9422",
   "metadata": {},
   "source": [
    "### Step 2: Lookup Coordinates for a Taxi Zone\n",
    "\n",
    "#### **What Happens in This Step**\n",
    "\n",
    "1. **Input Location ID**  \n",
    "   - The user provides a `LocationID`, a unique identifier for a specific taxi zone.\n",
    "\n",
    "2. **Filter the Data**  \n",
    "   - The program searches the GeoDataFrame to find the zone with the matching `LocationID`.\n",
    "\n",
    "3. **Extract the Geometry**  \n",
    "   - Once the zone is identified, its shape (geometry) is used to calculate the centroid (center point).\n",
    "\n",
    "4. **Output Coordinates**  \n",
    "   - The program returns the coordinates (latitude and longitude) of the centroid.  \n",
    "   - If the `LocationID` is not found in the data, it returns `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f3dabf-56d0-4d11-9b2f-0bc20e555b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    \"\"\"\n",
    "    Lookup the latitude and longitude for a given Taxi Zone Location ID.\n",
    "\n",
    "    Parameters:\n",
    "        zone_loc_id (int): Taxi Zone Location ID.\n",
    "        loaded_taxi_zones (gpd.GeoDataFrame): GeoDataFrame containing Taxi Zone data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (latitude, longitude) of the centroid of the zone geometry, \n",
    "               or None if the Location ID is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter GeoDataFrame for the given Location ID\n",
    "        zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "        \n",
    "        if zone.empty:\n",
    "            return None  # Return None if the Location ID is not found\n",
    "        \n",
    "        # Extract the geometry and calculate the centroid\n",
    "        centroid = zone.iloc[0].geometry.centroid\n",
    "        \n",
    "        # Return the centroid's coordinates as (latitude, longitude)\n",
    "        return centroid.y, centroid.x  # (latitude, longitude)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error looking up coordinates for Zone ID {zone_loc_id}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "408034ed-005e-4936-a93a-642db7c3f6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates for Zone ID 263: Latitude = 40.77876585543437, Longitude = -73.951009874818\n"
     ]
    }
   ],
   "source": [
    "# Lookup coordinates for a specific Taxi Zone ID\n",
    "zone_id = 263\n",
    "coords = lookup_coords_for_taxi_zone_id(zone_id, taxi_zones_gdf)\n",
    "\n",
    "if coords:\n",
    "    print(f\"Coordinates for Zone ID {zone_id}: Latitude = {coords[0]}, Longitude = {coords[1]}\")\n",
    "else:\n",
    "    print(f\"Zone ID {zone_id} not found in the Taxi Zone data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size\n",
    "This section demonstrates how to calculate the appropriate sample size for analyzing a dataset using Cochran's formula, specifically tailored for finite populations like taxi trip data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60091e1f-895d-4ee4-bbe9-eb086abc53d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To calculate the required sample size, we use **Cochran's formula**, which accounts for the population size and desired confidence level, margin of error, and proportion.\n",
    "\n",
    "1. **Input Parameters**  \n",
    "   - Population size: Total number of records in the dataset.  \n",
    "   - Confidence level: Typically 90%, 95%, or 99% (e.g., 95% confidence).  \n",
    "   - Margin of error: The acceptable error range for the results (e.g., ±5%).  \n",
    "   - Proportion: Estimated proportion of the population with the desired attribute (default is 0.5 for maximum variability).\n",
    "\n",
    "2. **Cochran's Formula for Infinite Populations**  \n",
    "   - The formula calculates the sample size assuming an infinite population:\n",
    "     $$\n",
    "     n_0 = \\frac{Z^2 \\cdot p \\cdot (1 - p)}{E^2}\n",
    "     $$\n",
    "     Where:\n",
    "     - \\( n_0 \\): Initial sample size for infinite population.\n",
    "     - \\( Z \\): Z-score based on the confidence level (e.g., 1.96 for 95% confidence).\n",
    "     - \\( p \\): Proportion of the population (default 0.5).  \n",
    "     - \\( E \\): Margin of error (e.g., 0.05 for ±5%).\n",
    "\n",
    "3. **Adjust for Finite Population Size**  \n",
    "   - If the population is finite, the sample size is adjusted using the correction formula:\n",
    "     $$\n",
    "     n = \\frac{n_0}{1 + \\frac{n_0 - 1}{N}}\n",
    "     $$\n",
    "     Where:\n",
    "     - \\( n \\): Final sample size for the finite population.  \n",
    "     - \\( N \\): Total population size.\n",
    "\n",
    "4. **Output**  \n",
    "   - The final sample size is rounded up to ensure a whole number of samples is selected.  \n",
    "   - This ensures the sample is statistically valid for the specified parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fb62f59-753b-45bb-bc0d-9d0c54e93ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population, confidence_level=0.95, margin_of_error=0.05, proportion=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the sample size using Cochran's formula for finite populations.\n",
    "\n",
    "    Parameters:\n",
    "        population (int): The size of the population (e.g., number of trips in a month).\n",
    "        confidence_level (float): The confidence level (default is 0.95 for 95% confidence).\n",
    "        margin_of_error (float): The margin of error (default is 0.05 for ±5% error).\n",
    "        proportion (float): The estimated proportion of the population with the desired attribute (default is 0.5).\n",
    "\n",
    "    Returns:\n",
    "        int: The calculated sample size.\n",
    "    \"\"\"\n",
    "    # Z-score for the given confidence level\n",
    "    z_scores = {0.9: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "    if confidence_level not in z_scores:\n",
    "        raise ValueError(\"Unsupported confidence level. Use 0.9, 0.95, or 0.99.\")\n",
    "    Z = z_scores[confidence_level]\n",
    "    \n",
    "    # Cochran's formula for an infinite population\n",
    "    n_0 = (Z**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust for finite population size\n",
    "    if population < n_0:\n",
    "        sample_size = population  # If the population is small, the entire dataset is needed\n",
    "    else:\n",
    "        sample_size = n_0 / (1 + (n_0 - 1) / population)\n",
    "    \n",
    "    return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population size: 6405008\n"
     ]
    }
   ],
   "source": [
    "yellow_taxi_df = pd.read_parquet(r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-01.parquet\")\n",
    "\n",
    "population_size = len(yellow_taxi_df)\n",
    "print(f\"Population size: {population_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db97f4-151b-4d90-8fe4-689157219647",
   "metadata": {},
   "source": [
    "### Step 1: Define a function `get_all_urls_from_tlc_page` that extracts all URLs from a specified webpage.\n",
    "\n",
    "1. **Input**:  \n",
    "   - The function takes a webpage URL (`taxi_page`) as input.\n",
    "\n",
    "2. **Request the Webpage**:  \n",
    "   - Using the `requests` library, the HTML content of the page is fetched.\n",
    "\n",
    "3. **Parse the HTML**:  \n",
    "   - The HTML content is parsed using `BeautifulSoup` from the `bs4` library.\n",
    "\n",
    "4. **Extract URLs**:  \n",
    "   - All anchor (`<a>`) tags with `href` attributes are located, and their links are extracted into a list.\n",
    "\n",
    "5. **Output**:  \n",
    "   - A list of extracted URLs is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_tlc_page(taxi_page):\n",
    "    \"\"\"\n",
    "    Extract all URLs from the TLC trip record data page.\n",
    "\n",
    "    Parameters:\n",
    "        taxi_page (str): URL of the TLC webpage.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of all extracted URLs from the webpage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Request the HTML page\n",
    "        response = requests.get(taxi_page)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all links on the page\n",
    "        links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error fetching or parsing the TLC page: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aabb8faf-c83d-489a-8bb9-91193037876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 563 URLs:\n",
      "http://www1.nyc.gov\n",
      "/311/index.page\n",
      "/home/search/index.page\n",
      "#\n",
      "/site/tlc/index.page\n",
      "http://www1.nyc.gov/home/text-size.page\n",
      "#\n",
      "/site/tlc/index.page\n",
      "/site/tlc/about/about-tlc.page\n",
      "/site/tlc/passengers/your-ride.page\n",
      "/site/tlc/drivers/get-a-drivers-license.page\n",
      "/site/tlc/vehicles/get-a-vehicle-license.page\n",
      "/site/tlc/businesses/yellow-cab.page\n",
      "/site/tlc/tlc-online/tlc-online.page\n",
      "/site/tlc/about/about-tlc.page\n",
      "/site/tlc/about/data-and-research.page\n",
      "/site/tlc/about/tlc-initiatives.page\n",
      "/site/tlc/about/contact-tlc.page\n",
      "/site/tlc/about/data.page\n",
      "/site/tlc/about/pilot-programs.page\n",
      "/site/tlc/about/industry-reports.page\n",
      "/site/tlc/about/tlc-trip-record-data.page\n",
      "/site/tlc/about/request-data.page\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "javascript:expandAll();\n",
      "javascript:collapseAll();\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-02.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-08.parquet\n",
      " https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2024-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2023-12.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2020-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2020-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2019-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2018-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2018-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2018-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2017-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2017-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2016-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-12.parquet\n",
      "/assets/tlc/downloads/pdf/trip_record_user_guide.pdf\n",
      "/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\n",
      "/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf\n",
      "/assets/tlc/downloads/pdf/data_dictionary_trip_records_fhv.pdf\n",
      "/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf\n",
      "/assets/tlc/downloads/pdf/working_parquet_format.pdf\n",
      "https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip\n",
      "/assets/tlc/images/content/pages/about/taxi_zone_map_bronx.jpg\n",
      "/assets/tlc/images/content/pages/about/taxi_zone_map_brooklyn.jpg\n",
      "/assets/tlc/images/content/pages/about/taxi_zone_map_manhattan.jpg\n",
      "/assets/tlc/images/content/pages/about/taxi_zone_map_queens.jpg\n",
      "/assets/tlc/images/content/pages/about/taxi_zone_map_staten_island.jpg\n",
      "/nyc-resources/agencies.page\n",
      "/home/contact-us.page\n",
      "https://a127-ess.nyc.gov\n",
      "https://a858-nycnotify.nyc.gov/notifynyc/\n",
      "https://a856-citystore.nyc.gov/\n",
      "/connect/social-media.page\n",
      "/connect/mobile-applications.page\n",
      "/nyc-resources/nyc-maps.page\n",
      "/nyc-resources/resident-toolkit.page\n",
      "/home/privacy-policy.page\n",
      "/home/terms-of-use.page\n",
      "https://www.nyc.gov/nyc-resources/website-accessibility-statement.page\n"
     ]
    }
   ],
   "source": [
    "# TLC Trip Record Data Page URL\n",
    "URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Extract all URLs\n",
    "try:\n",
    "    all_urls = get_all_urls_from_tlc_page(URL)\n",
    "    print(f\"Extracted {len(all_urls)} URLs:\")\n",
    "    for url in all_urls:  # Print all links in  URL\n",
    "        print(url)\n",
    "except RuntimeError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71663e7b-38d8-4636-8a39-59e88a5d1a05",
   "metadata": {},
   "source": [
    "### Step 2: The `filter_parquet_urls` function uses regular expressions to match and filter URLs for Yellow Taxi and HVFHV datasets. \n",
    "\n",
    "1. **Input**:  \n",
    "   - A list of URLs (`all_urls`) extracted from the TLC page.\n",
    "\n",
    "2. **Regular Expressions**:  \n",
    "   - Patterns are defined to match Parquet files for:\n",
    "     - Yellow Taxi datasets (`yellow_tripdata`).\n",
    "     - HVFHV datasets (`fhvhv_tripdata`).\n",
    "   - The date ranges included are:\n",
    "     - 2020 (January to December).\n",
    "     - 2021–2023 (all months).\n",
    "     - 2024 (January to August).\n",
    "\n",
    "3. **Filter URLs**:  \n",
    "   - URLs matching the patterns are extracted into separate lists.\n",
    "\n",
    "4. **Output**:  \n",
    "   - Two lists: one for Yellow Taxi dataset links, and another for HVFHV dataset links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(all_urls):\n",
    "    \"\"\"\n",
    "    Filter Yellow Taxi and HVFHV Parquet file URLs for the specified date range.\n",
    "\n",
    "    Parameters:\n",
    "        all_urls (list): A list of all URLs extracted from the TLC page.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two lists containing filtered URLs for Yellow Taxi and HVFHV datasets.\n",
    "    \"\"\"\n",
    "    # Compile regular expressions to match the desired Parquet files\n",
    "    yellow_taxi_pattern = re.compile(\n",
    "        r'yellow_tripdata_2020-(0[1-9]|1[0-2])\\.parquet|'\n",
    "        r'yellow_tripdata_202[1-3]-\\d{2}\\.parquet|'\n",
    "        r'yellow_tripdata_2024-(0[1-8])\\.parquet'\n",
    "    )\n",
    "    fhvhv_pattern = re.compile(\n",
    "        r'fhvhv_tripdata_2020-(0[1-9]|1[0-2])\\.parquet|'\n",
    "        r'fhvhv_tripdata_202[1-3]-\\d{2}\\.parquet|'\n",
    "        r'fhvhv_tripdata_2024-(0[1-8])\\.parquet'\n",
    "    )\n",
    "\n",
    "    # Filter URLs using the patterns\n",
    "    yellow_taxi_links = [url for url in all_urls if yellow_taxi_pattern.search(url)]\n",
    "    fhvhv_links = [url for url in all_urls if fhvhv_pattern.search(url)]\n",
    "\n",
    "    return yellow_taxi_links, fhvhv_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d360f474-3f1c-448e-87e0-9bd9c1c948d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Taxi Links:\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet\n",
      "\n",
      "FHVHV Links:\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet \n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-12.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-02.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-03.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-04.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-05.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-06.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-07.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-08.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-09.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-10.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-11.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-12.parquet\n"
     ]
    }
   ],
   "source": [
    "# Example: Extract URLs from the TLC page\n",
    "all_urls = get_all_urls_from_tlc_page(\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\")\n",
    "\n",
    "# Filter the Parquet file URLs\n",
    "yellow_taxi_links, fhvhv_links = filter_parquet_urls(all_urls)\n",
    "\n",
    "# Print the filtered links\n",
    "print(\"Yellow Taxi Links:\")\n",
    "for link in yellow_taxi_links:\n",
    "    print(link)\n",
    "\n",
    "print(\"\\nFHVHV Links:\")\n",
    "for link in fhvhv_links:\n",
    "    print(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf2ded-aa8d-4f60-9229-2cae7020b02b",
   "metadata": {},
   "source": [
    "### Step 3: Define the Download Function\n",
    "\n",
    "The `download_files` function takes a list of file URLs and downloads them to a specified directory.\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `file_links`: A list of URLs for the files to download.\n",
    "   - `save_dir`: Path to the directory where files will be saved.\n",
    "\n",
    "2. **Ensure Save Directory Exists**:\n",
    "   - The function creates the directory if it does not already exist using `os.makedirs`.\n",
    "\n",
    "3. **Download Files**:\n",
    "   - For each URL in `file_links`:\n",
    "     - The file name is extracted from the URL.\n",
    "     - The file is downloaded using `requests` and saved locally in the specified directory.\n",
    "     - The function prints the status of each download.\n",
    "\n",
    "4. **Handle Errors**:\n",
    "   - If any error occurs during the download, an appropriate error message is printed.\n",
    "\n",
    "5. **Use Download Function**:\n",
    "   - we use download function to download parquet file from yellow_taxi_links, fhvhv_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "419da674-f577-4e95-a8b7-5d170be9ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(file_links, save_dir):\n",
    "    \"\"\"\n",
    "    Downloads files from a list of URLs and saves them to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        file_links (list): List of URLs for the files to download.\n",
    "        save_dir (str): Path to the directory where files will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for link in file_links:\n",
    "        try:\n",
    "            # Extract file name from the URL\n",
    "            file_name = link.split(\"/\")[-1]\n",
    "            file_path = os.path.join(save_dir, file_name)\n",
    "            \n",
    "            # Download the file\n",
    "            print(f\"Downloading {file_name}...\")\n",
    "            response = requests.get(link, stream=True)\n",
    "            response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "            # Save the file locally\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(f\"Downloaded: {file_name}\")\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download {link}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41021bb6-eeff-4a61-b1c0-b1ebd817f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "file_links = [yellow_taxi_links]\n",
    "\n",
    "save_dir = \"C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\"\n",
    "download_files(file_links, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e2acd-7ba0-4234-b83c-501d37c0baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_links = fhvhv_links  # Use the list directly, assuming fhvhv_links is already a list of URLs\n",
    "save_dir = \"C:\\Users\\wenji\\OneDrive\\Project\\downloads_HVFHV\"\n",
    "\n",
    "download_files(file_links, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: Define the Sampling Function for a Single Parquet File\n",
    "\n",
    "The `sample_parquet_file` function loads a Parquet file, calculates the required sample size using Cochran's formula, extracts a random sample of rows, and saves the sampled data to a new Parquet file.\n",
    "\n",
    "1. **Process**:\n",
    "   - Loads the dataset from the specified Parquet file.\n",
    "   - Calculates the sample size using the `calculate_sample_size` function.\n",
    "   - Extracts a random sample of rows if the sample size is less than the population size. Otherwise, uses the full dataset.\n",
    "   - Saves the sampled data as a new Parquet file in the output directory.\n",
    "\n",
    "2. **Output**:\n",
    "   - A new Parquet file containing the sampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_parquet_file(parquet_file, output_directory=\"sampled yellow taxi\", confidence_level=0.95, margin_of_error=0.05, proportion=0.5):\n",
    "    \"\"\"\n",
    "    Load a Parquet file, calculate a sample size, extract a random sample,\n",
    "    and save it to a new Parquet file in the 'sampled yellow taxi' folder.\n",
    "\n",
    "    Parameters:\n",
    "        parquet_file (str): Path to the input Parquet file.\n",
    "        output_directory (str): Directory to save the sampled Parquet file.\n",
    "        confidence_level (float): Confidence level for sample size calculation (default is 0.95).\n",
    "        margin_of_error (float): Margin of error for sample size calculation (default is 0.05).\n",
    "        proportion (float): Estimated proportion of the population with the desired attribute (default is 0.5).\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Load the dataset\n",
    "    print(f\"Loading Parquet file: {parquet_file}\")\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "\n",
    "    # Calculate sample size\n",
    "    population_size = len(df)\n",
    "    sample_size = calculate_sample_size(population_size, confidence_level, margin_of_error, proportion)\n",
    "    print(f\"Population size: {population_size}, Sample size: {sample_size}\")\n",
    "\n",
    "    # If the sample size is larger than the population, use the full dataset\n",
    "    if sample_size >= population_size:\n",
    "        print(\"Sample size is larger than or equal to the population. Using the entire dataset.\")\n",
    "        sampled_df = df\n",
    "    else:\n",
    "        # Extract random sample\n",
    "        print(f\"Sampling {sample_size} rows from the dataset...\")\n",
    "        sampled_df = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    # Save the sampled dataset\n",
    "    file_name = os.path.basename(parquet_file)\n",
    "    output_file = os.path.join(output_directory, os.path.splitext(file_name)[0] + \"_sample.parquet\")\n",
    "    print(f\"Saving sampled data to: {output_file}\")\n",
    "    sampled_df.to_parquet(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e13f9-9f7e-4490-9e40-ea9c708ad2ce",
   "metadata": {},
   "source": [
    "### Step 5: Apply Sampling to All Files in a Directory\n",
    "\n",
    "The `sample_all_parquet_files` function applies the `sample_parquet_file` function to all Parquet files in a specified directory.\n",
    "\n",
    "1. **Process**:\n",
    "   - Ensures the `output_directory` exists or creates it.\n",
    "   - Iterates through all files in the `input_directory`.\n",
    "   - Identifies Parquet files by their `.parquet` extension.\n",
    "   - Applies the `sample_parquet_file` function to each Parquet file.\n",
    "\n",
    "2. **Output**:\n",
    "   - Sampled datasets are saved as new Parquet files in the `output_directory`, with filenames suffixed by `_sample.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_all_parquet_files(input_directory, output_directory=\"sampled yellow taxi\", confidence_level=0.95, margin_of_error=0.05, proportion=0.5):\n",
    "    \"\"\"\n",
    "    Apply sample_parquet_file to all Parquet files in a directory.\n",
    "\n",
    "    Parameters:\n",
    "        input_directory (str): Directory containing the Parquet files.\n",
    "        output_directory (str): Directory to save the sampled Parquet files.\n",
    "        confidence_level (float): Confidence level for sample size calculation (default is 0.95).\n",
    "        margin_of_error (float): Margin of error for sample size calculation (default is 0.05).\n",
    "        proportion (float): Estimated proportion of the population with the desired attribute (default is 0.5).\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Iterate through all files in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".parquet\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            # Apply sampling to each file\n",
    "            try:\n",
    "                sample_parquet_file(file_path, output_directory, confidence_level, margin_of_error, proportion)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process file {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "200776ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-01.parquet\n",
      "Population size: 6405008, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-02.parquet\n",
      "Population size: 6299367, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-03.parquet\n",
      "Population size: 3007687, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-04.parquet\n",
      "Population size: 238073, Sample size: 384\n",
      "Sampling 384 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-05.parquet\n",
      "Population size: 348415, Sample size: 384\n",
      "Sampling 384 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-06.parquet\n",
      "Population size: 549797, Sample size: 384\n",
      "Sampling 384 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-07.parquet\n",
      "Population size: 800412, Sample size: 384\n",
      "Sampling 384 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-08.parquet\n",
      "Population size: 1007286, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-09.parquet\n",
      "Population size: 1341017, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-10.parquet\n",
      "Population size: 1681132, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-11.parquet\n",
      "Population size: 1509000, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2020-12.parquet\n",
      "Population size: 1461898, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-12_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-01.parquet\n",
      "Population size: 1369769, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-02.parquet\n",
      "Population size: 1371709, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-03.parquet\n",
      "Population size: 1925152, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-04.parquet\n",
      "Population size: 2171187, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-05.parquet\n",
      "Population size: 2507109, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-06.parquet\n",
      "Population size: 2834264, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-07.parquet\n",
      "Population size: 2821746, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-08.parquet\n",
      "Population size: 2788757, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-09.parquet\n",
      "Population size: 2963793, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-10.parquet\n",
      "Population size: 3463504, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-11.parquet\n",
      "Population size: 3472949, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2021-12.parquet\n",
      "Population size: 3214369, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-12_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-01.parquet\n",
      "Population size: 2463931, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-02.parquet\n",
      "Population size: 2979431, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-03.parquet\n",
      "Population size: 3627882, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-04.parquet\n",
      "Population size: 3599920, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-05.parquet\n",
      "Population size: 3588295, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-06.parquet\n",
      "Population size: 3558124, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-07.parquet\n",
      "Population size: 3174394, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-08.parquet\n",
      "Population size: 3152677, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-09.parquet\n",
      "Population size: 3183767, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-10.parquet\n",
      "Population size: 3675411, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-11.parquet\n",
      "Population size: 3252717, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2022-12.parquet\n",
      "Population size: 3399549, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-12_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-01.parquet\n",
      "Population size: 3066766, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-02.parquet\n",
      "Population size: 2913955, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-03.parquet\n",
      "Population size: 3403766, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-04.parquet\n",
      "Population size: 3288250, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-05.parquet\n",
      "Population size: 3513649, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-06.parquet\n",
      "Population size: 3307234, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-07.parquet\n",
      "Population size: 2907108, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-08.parquet\n",
      "Population size: 2824209, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-08_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-09.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-09.parquet\n",
      "Population size: 2846722, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-09_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-10.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-10.parquet\n",
      "Population size: 3522285, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-10_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-11.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-11.parquet\n",
      "Population size: 3339715, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-11_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-12.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2023-12.parquet\n",
      "Population size: 3376567, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-12_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-01.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-01.parquet\n",
      "Population size: 2964624, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-01_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-02.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-02.parquet\n",
      "Population size: 3007526, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-02_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-03.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-03.parquet\n",
      "Population size: 3582628, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-03_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-04.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-04.parquet\n",
      "Population size: 3514289, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-04_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-05.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-05.parquet\n",
      "Population size: 3723833, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-05_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-06.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-06.parquet\n",
      "Population size: 3539193, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-06_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-07.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-07.parquet\n",
      "Population size: 3076903, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-07_sample.parquet\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-08.parquet\n",
      "Loading Parquet file: C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\\yellow_tripdata_2024-08.parquet\n",
      "Population size: 2979183, Sample size: 385\n",
      "Sampling 385 rows from the dataset...\n",
      "Saving sampled data to: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-08_sample.parquet\n"
     ]
    }
   ],
   "source": [
    "input_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\downloads yellow taxi\"\n",
    "output_directory = r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\"\n",
    "\n",
    "# Apply sampling to all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104b4da-32c3-4f9e-a805-24b4d0e2cbc3",
   "metadata": {},
   "source": [
    "### Process Taxi data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3e7ea-6ba4-4d58-8d51-aa37d6250e98",
   "metadata": {},
   "source": [
    "### Step 1: Define the Cleaning Function\n",
    "\n",
    "The `clean_parquet_columns` function reads a Parquet file, selects specific columns, and returns the cleaned DataFrame.\n",
    "\n",
    "1. **Input**:\n",
    "   - `parquet_file`: Path to the Parquet file that needs cleaning.\n",
    "\n",
    "2. **Process**:\n",
    "   - Reads the Parquet file into a Pandas DataFrame.\n",
    "   - Specifies the relevant columns to keep:\n",
    "     - `'tpep_pickup_datetime'`: Pickup time.\n",
    "     - `'tpep_dropoff_datetime'`: Dropoff time.\n",
    "     - `'passenger_count'`: Number of passengers.\n",
    "     - `'trip_distance'`: Distance of the trip.\n",
    "     - `'PULocationID'`: Pickup location ID.\n",
    "     - `'DOLocationID'`: Dropoff location ID.\n",
    "     - `'tip_amount'`: Amount of tip given.\n",
    "     - `'total_amount'`: Total amount charged.\n",
    "     - `'congestion_surcharge'`: Additional congestion surcharge.\n",
    "   - Filters the DataFrame to include only these columns.\n",
    "\n",
    "3. **Output**:\n",
    "   - A cleaned DataFrame containing only the specified columns.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - If an error occurs during processing, the function prints the error message and returns `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parquet_columns(parquet_file):\n",
    "    \"\"\"\n",
    "    Read a Parquet file, keep only relevant columns, and return the cleaned DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - parquet_file (str): Path to the Parquet file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Parquet file\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "\n",
    "        # Specify the columns to keep\n",
    "        columns_to_keep = [\n",
    "            'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "            'passenger_count', 'trip_distance',\n",
    "            'PULocationID', 'DOLocationID', 'tip_amount', 'total_amount', 'congestion_surcharge'\n",
    "        ]\n",
    "\n",
    "        # Filter the DataFrame to keep only the specified columns\n",
    "        df_cleaned = df[columns_to_keep]\n",
    "        return df_cleaned\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the Parquet file: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2020-02-19 19:37:08   2020-02-19 19:42:59              1.0           1.24   \n",
      "1  2020-02-27 12:17:32   2020-02-27 12:26:00              1.0           0.86   \n",
      "2  2020-02-06 14:38:29   2020-02-06 14:44:46              1.0           1.40   \n",
      "3  2020-02-01 12:10:41   2020-02-01 12:22:25              1.0           1.80   \n",
      "4  2020-02-14 19:18:29   2020-02-14 19:37:45              3.0           1.85   \n",
      "5  2020-02-17 16:02:28   2020-02-17 16:09:40              1.0           1.90   \n",
      "6  2020-02-08 12:38:37   2020-02-08 12:51:12              1.0           1.74   \n",
      "7  2020-02-23 19:40:40   2020-02-23 19:48:55              1.0           1.31   \n",
      "8  2020-02-01 04:43:14   2020-02-01 04:53:06              1.0           2.10   \n",
      "9  2020-02-06 17:27:39   2020-02-06 17:43:09              1.0           3.13   \n",
      "\n",
      "   PULocationID  DOLocationID  tip_amount  total_amount  congestion_surcharge  \n",
      "0           237           236        0.00         10.30                   2.5  \n",
      "1           237           237        0.00         10.30                   2.5  \n",
      "2            48           143        2.00         12.30                   2.5  \n",
      "3           142           140        2.56         15.36                   2.5  \n",
      "4           246           137        3.36         20.16                   2.5  \n",
      "5            68            50        2.15         12.95                   2.5  \n",
      "6           237           164        2.56         15.36                   2.5  \n",
      "7            48           143        0.00         10.80                   2.5  \n",
      "8            79           164        2.00         15.30                   2.5  \n",
      "9           143            68        4.33         21.63                   2.5  \n"
     ]
    }
   ],
   "source": [
    "file= r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-02_sample.parquet\"\n",
    "clean = clean_parquet_columns(file)\n",
    "print(clean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c92ba-568e-463a-9e1f-e184d3ca41c9",
   "metadata": {},
   "source": [
    "### Step 2: Define the Normalization Function\n",
    "\n",
    "The `normalize_column_names` function modifies column names in the following ways:\n",
    "1. **Convert to Lowercase**:\n",
    "   - Ensures all column names are in lowercase for uniformity.\n",
    "2. **Remove Special Characters**:\n",
    "   - Strips out non-alphanumeric characters except for underscores.\n",
    "3. **Replace Spaces with Underscores**:\n",
    "   - Converts spaces and other whitespace to underscores.\n",
    "4. **Strip Leading and Trailing Spaces**:\n",
    "   - Removes any extra spaces around column names.\n",
    "\n",
    "#### **Input**:\n",
    "- `df`: A Pandas DataFrame with column names to be normalized.\n",
    "\n",
    "#### **Output**:\n",
    "- A DataFrame with normalized column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "344c5e3f-4ab8-4762-8cec-6e474cd01b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column_names(df):\n",
    "    \"\"\"\n",
    "    Normalize column names in a DataFrame by:\n",
    "    - Converting to lowercase\n",
    "    - Replacing spaces and special characters with underscores\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with normalized column names.\n",
    "    \"\"\"\n",
    "    # Normalize column names\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()               # Remove leading/trailing spaces\n",
    "        .str.lower()               # Convert to lowercase\n",
    "        .str.replace(r'[^\\w\\s]', '', regex=True)  # Remove special characters\n",
    "        .str.replace(r'\\s+', '_', regex=True)     # Replace spaces with underscores\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "475c4582-4d59-493f-a5c8-3e25679698fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2020-02-19 19:37:08   2020-02-19 19:42:59              1.0           1.24   \n",
      "1  2020-02-27 12:17:32   2020-02-27 12:26:00              1.0           0.86   \n",
      "2  2020-02-06 14:38:29   2020-02-06 14:44:46              1.0           1.40   \n",
      "3  2020-02-01 12:10:41   2020-02-01 12:22:25              1.0           1.80   \n",
      "4  2020-02-14 19:18:29   2020-02-14 19:37:45              3.0           1.85   \n",
      "5  2020-02-17 16:02:28   2020-02-17 16:09:40              1.0           1.90   \n",
      "6  2020-02-08 12:38:37   2020-02-08 12:51:12              1.0           1.74   \n",
      "7  2020-02-23 19:40:40   2020-02-23 19:48:55              1.0           1.31   \n",
      "8  2020-02-01 04:43:14   2020-02-01 04:53:06              1.0           2.10   \n",
      "9  2020-02-06 17:27:39   2020-02-06 17:43:09              1.0           3.13   \n",
      "\n",
      "   pulocationid  dolocationid  tip_amount  total_amount  congestion_surcharge  \n",
      "0           237           236        0.00         10.30                   2.5  \n",
      "1           237           237        0.00         10.30                   2.5  \n",
      "2            48           143        2.00         12.30                   2.5  \n",
      "3           142           140        2.56         15.36                   2.5  \n",
      "4           246           137        3.36         20.16                   2.5  \n",
      "5            68            50        2.15         12.95                   2.5  \n",
      "6           237           164        2.56         15.36                   2.5  \n",
      "7            48           143        0.00         10.80                   2.5  \n",
      "8            79           164        2.00         15.30                   2.5  \n",
      "9           143            68        4.33         21.63                   2.5  \n"
     ]
    }
   ],
   "source": [
    "clean1 = normalize_column_names(clean)\n",
    "print(clean1.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e7aa7-fbbd-4c02-a193-4271e3b4af65",
   "metadata": {},
   "source": [
    "### Step 3: Define the Processing Function\n",
    "This section explains how to process a taxi trip dataset by cleaning, mapping location IDs to geographic coordinates, and filtering the data based on a bounding box.\n",
    "The `process_taxi_data_from_df` function processes a taxi trip dataset in the following steps:\n",
    "\n",
    "\n",
    "1. **Processing Steps**:\n",
    "   - **Normalize Column Names**:\n",
    "     - Standardizes column names to lowercase and replaces special characters with underscores.\n",
    "   - **Filter Invalid Location IDs**:\n",
    "     - Removes rows where `PULocationID` or `DOLocationID` is missing or outside the valid range (1–263).\n",
    "   - **Map Location IDs to Coordinates**:\n",
    "     - Maps pickup and dropoff location IDs to their respective latitude and longitude using the Taxi Zone GeoDataFrame.\n",
    "   - **Split Coordinates**:\n",
    "     - Adds separate columns for pickup and dropoff latitude/longitude.\n",
    "   - **Filter Rows by Bounding Box**:\n",
    "     - Keeps rows with valid pickup and dropoff coordinates within the defined geographic bounding box.\n",
    "\n",
    "2. **Output**:\n",
    "   - A cleaned and filtered DataFrame ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1f7babd-c468-4810-a790-2c4e059bd337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_taxi_data_from_df(df, loaded_taxi_zones, lat_min=40.560445, lon_min=-74.242330, lat_max=40.908524, lon_max=-73.717047):\n",
    "    \"\"\"\n",
    "    Process a DataFrame by cleaning and filtering based on PULocationID and DOLocationID.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        loaded_taxi_zones (gpd.GeoDataFrame): GeoDataFrame containing Taxi Zone data.\n",
    "        lat_min (float): Minimum latitude for the bounding box.\n",
    "        lon_min (float): Minimum longitude for the bounding box.\n",
    "        lat_max (float): Maximum latitude for the bounding box.\n",
    "        lon_max (float): Maximum longitude for the bounding box.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with valid coordinates.\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid warnings\n",
    "    df = df.copy()\n",
    "    print(f\"Processing DataFrame with {len(df)} rows.\")\n",
    "\n",
    "    # Normalize column names\n",
    "    df = normalize_column_names(df)\n",
    "\n",
    "    # Step 1: Drop rows where PULocationID or DOLocationID is NaN or not in range 1-263\n",
    "    valid_ids = range(1, 264)\n",
    "    df = df[\n",
    "        df['pulocationid'].isin(valid_ids) &\n",
    "        df['dolocationid'].isin(valid_ids)\n",
    "    ]\n",
    "    print(f\"After removing invalid location IDs: {len(df)} rows remaining.\")\n",
    "\n",
    "    # Step 2: Map PULocationID and DOLocationID to coordinates\n",
    "    def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "        try:\n",
    "            zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "            if zone.empty:\n",
    "                return None\n",
    "            centroid = zone.iloc[0].geometry.centroid\n",
    "            return centroid.y, centroid.x\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error looking up coordinates for Zone ID {zone_loc_id}: {e}\")\n",
    "\n",
    "    df.loc[:, 'pickup_coords'] = df['pulocationid'].apply(partial(lookup_coords_for_taxi_zone_id, loaded_taxi_zones=loaded_taxi_zones))\n",
    "    df.loc[:, 'dropoff_coords'] = df['dolocationid'].apply(partial(lookup_coords_for_taxi_zone_id, loaded_taxi_zones=loaded_taxi_zones))\n",
    "\n",
    "    # Split coordinates into separate latitude and longitude columns\n",
    "    pickup_coords_df = pd.DataFrame(df['pickup_coords'].tolist(), columns=['pickup_latitude', 'pickup_longitude'])\n",
    "    dropoff_coords_df = pd.DataFrame(df['dropoff_coords'].tolist(), columns=['dropoff_latitude', 'dropoff_longitude'])\n",
    "\n",
    "    df = pd.concat([df, pickup_coords_df, dropoff_coords_df], axis=1)\n",
    "\n",
    "    # Drop rows where mapping failed (NaN coordinates)\n",
    "    df = df.dropna(subset=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'])\n",
    "    print(f\"After mapping location IDs to coordinates: {len(df)} rows remaining.\")\n",
    "\n",
    "    # Step 3: Filter rows based on the bounding box\n",
    "    df = df[\n",
    "        (df['pickup_latitude'].between(lat_min, lat_max)) &\n",
    "        (df['pickup_longitude'].between(lon_min, lon_max)) &\n",
    "        (df['dropoff_latitude'].between(lat_min, lat_max)) &\n",
    "        (df['dropoff_longitude'].between(lon_min, lon_max))\n",
    "    ]\n",
    "    print(f\"After filtering by bounding box: {len(df)} rows remaining.\")\n",
    "\n",
    "    # Drop intermediate columns if not needed\n",
    "    df = df.drop(columns=['pickup_coords', 'dropoff_coords'])\n",
    "\n",
    "    print(\"Processing completed.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed261d-c4f2-46bd-a6fa-291739ad0700",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 4: Define Total_clean Function\n",
    "\n",
    "The `total_clean_taxi_data` function combines previously defined steps into a streamlined process for data cleaning and filtering.\n",
    "\n",
    "#### **Processing Steps**:\n",
    "1. **Clean Parquet Columns**:\n",
    "   - Reads the Parquet file and retains only the relevant columns, such as pickup/dropoff timestamps, passenger count, trip distance, and location IDs.\n",
    "2. **Normalize Column Names**:\n",
    "   - Converts column names to lowercase and replaces spaces/special characters with underscores for consistency.\n",
    "3. **Filter and Map Data**:\n",
    "   - Maps location IDs to geographic coordinates, filters invalid rows, and applies a bounding box to keep rows within the NYC area.\n",
    "\n",
    "#### **Output**:\n",
    "- A cleaned and filtered DataFrame ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb0fead3-650d-4e65-a6ef-d765c97b835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a reference to a function that I wrote separately in the three steps above, and put it all together.\n",
    "def total_clean_taxi_data(file, taxi_zones_gdf):\n",
    "    \"\"\"\n",
    "    Process a Parquet file by cleaning, normalizing, and filtering data.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): Path to the Parquet file.\n",
    "        taxi_zones_gdf (gpd.GeoDataFrame): GeoDataFrame containing Taxi Zone data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed and filtered DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing file: {file}\")\n",
    "\n",
    "        # Step 1: Clean the Parquet columns\n",
    "        cleaned_df = clean_parquet_columns(file)\n",
    "        if cleaned_df is None:\n",
    "            raise ValueError(\"Cleaning Parquet columns failed.\")\n",
    "\n",
    "        # Step 2: Normalize column names\n",
    "        normalized_df = normalize_column_names(cleaned_df)\n",
    "\n",
    "        # Step 3: Process the taxi data\n",
    "        processed_df = process_taxi_data_from_df(normalized_df, taxi_zones_gdf)\n",
    "\n",
    "        print(\"Processing completed.\")\n",
    "        return processed_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1878a4b-c7aa-47f6-af78-a7d0f8110318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-09_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 382 rows remaining.\n",
      "After mapping location IDs to coordinates: 382 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0  2021-09-03 13:12:41   2021-09-03 13:19:43              1.0           1.01   \n",
      "1  2021-09-14 17:57:35   2021-09-14 18:19:34              1.0           2.67   \n",
      "2  2021-09-18 06:47:25   2021-09-18 06:52:03              1.0           1.04   \n",
      "3  2021-09-20 10:32:05   2021-09-20 10:40:46              1.0           1.04   \n",
      "4  2021-09-25 11:59:50   2021-09-25 12:00:08              1.0           0.00   \n",
      "5  2021-09-19 10:56:31   2021-09-19 11:07:40              2.0           2.00   \n",
      "6  2021-09-01 16:14:18   2021-09-01 16:22:33              1.0           1.00   \n",
      "7  2021-09-22 17:45:40   2021-09-22 17:54:51              1.0           1.32   \n",
      "8  2021-09-25 20:29:40   2021-09-25 20:35:04              1.0           0.75   \n",
      "9  2021-09-02 02:43:24   2021-09-02 02:53:43              1.0           3.09   \n",
      "\n",
      "   pulocationid  dolocationid  tip_amount  total_amount  congestion_surcharge  \\\n",
      "0         237.0          43.0        1.86         11.16                   2.5   \n",
      "1          79.0         246.0        1.00         20.30                   2.5   \n",
      "2         237.0         236.0        0.00          9.30                   2.5   \n",
      "3         236.0         237.0        2.06         12.36                   2.5   \n",
      "4         141.0         141.0        0.00          5.80                   2.5   \n",
      "5          68.0         230.0        0.00         12.80                   2.5   \n",
      "6         236.0         141.0        2.03         13.33                   2.5   \n",
      "7         142.0         237.0        0.00         11.80                   2.5   \n",
      "8          90.0          68.0        1.86         11.16                   2.5   \n",
      "9         148.0         255.0        3.06         18.36                   2.5   \n",
      "\n",
      "   pickup_latitude  pickup_longitude  dropoff_latitude  dropoff_longitude  \n",
      "0        40.768615        -73.965635         40.782478         -73.965554  \n",
      "1        40.727620        -73.985937         40.753309         -74.004015  \n",
      "2        40.768615        -73.965635         40.780436         -73.957012  \n",
      "3        40.780436        -73.957012         40.768615         -73.965635  \n",
      "4        40.766948        -73.959635         40.766948         -73.959635  \n",
      "5        40.748428        -73.999917         40.759818         -73.984196  \n",
      "6        40.780436        -73.957012         40.766948         -73.959635  \n",
      "7        40.773633        -73.981532         40.768615         -73.965635  \n",
      "8        40.742279        -73.996971         40.748428         -73.999917  \n",
      "9        40.718938        -73.990896         40.718804         -73.957418  \n"
     ]
    }
   ],
   "source": [
    "file= r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-09_sample.parquet\"\n",
    "ok= total_clean_taxi_data(file, taxi_zones_gdf)\n",
    "print(ok.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4a660-00a8-4376-9762-9d2199380f28",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 5: Define the merge files Function `process_and_merge_files`\n",
    "\n",
    "The `process_and_merge_files` function combines multiple DataFrames generated from taxi trip data files into a single consolidated DataFrame.\n",
    "\n",
    "\n",
    "#### **Processing Steps**:\n",
    "1. **Iterate Through Files**:\n",
    "   - Reads each file in the directory.\n",
    "   - Ensures the file is valid before processing.\n",
    "2. **Apply Cleaning and Filtering**:\n",
    "   - Uses the `total_clean_taxi_data` function to clean and filter each file.\n",
    "   - Appends the processed data to a list.\n",
    "3. **Merge DataFrames**:\n",
    "   - Combines all processed DataFrames into a single DataFrame.\n",
    "   - Ensures unique indices by resetting the index during concatenation.\n",
    "4. **Output**:\n",
    "   - Returns the merged DataFrame containing all processed rows.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3469adbc-045c-41c9-ba32-a71d0ce9ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all dataframe that generate from parquet \n",
    "def process_and_merge_files(directory, taxi_zones_gdf):\n",
    "    \"\"\"\n",
    "    Process all files in a directory using total_clean_taxi_data, \n",
    "    and merge the resulting DataFrames where each processed file \n",
    "    contributes one row to the final DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing the files.\n",
    "        taxi_zones_gdf (GeoDataFrame): The taxi zones GeoDataFrame required by total_clean_taxi_data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A merged DataFrame with unique indices.\n",
    "    \"\"\"\n",
    "    all_data = []  # List to collect processed DataFrames\n",
    "    \n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.isfile(file_path):  # Ensure it's a file\n",
    "            try:\n",
    "                # Process the file using total_clean_taxi_data\n",
    "                processed_data = total_clean_taxi_data(file_path, taxi_zones_gdf)\n",
    "                if isinstance(processed_data, pd.DataFrame):\n",
    "                    # Add the resulting DataFrame with reset index to avoid duplicate indices\n",
    "                    all_data.append(processed_data.reset_index(drop=True))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "    \n",
    "    # Combine all DataFrames into a single DataFrame\n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)  # Ensure final DataFrame has unique indices\n",
    "    else:\n",
    "        final_df = pd.DataFrame()  # Return an empty DataFrame if no files were processed\n",
    "    \n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e2d70f2-3d72-4602-939b-337a3151e82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-01_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-02_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 382 rows remaining.\n",
      "After mapping location IDs to coordinates: 382 rows remaining.\n",
      "After filtering by bounding box: 382 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-03_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-04_sample.parquet\n",
      "Processing DataFrame with 384 rows.\n",
      "After removing invalid location IDs: 383 rows remaining.\n",
      "After mapping location IDs to coordinates: 383 rows remaining.\n",
      "After filtering by bounding box: 383 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-05_sample.parquet\n",
      "Processing DataFrame with 384 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-06_sample.parquet\n",
      "Processing DataFrame with 384 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-07_sample.parquet\n",
      "Processing DataFrame with 384 rows.\n",
      "After removing invalid location IDs: 382 rows remaining.\n",
      "After mapping location IDs to coordinates: 382 rows remaining.\n",
      "After filtering by bounding box: 382 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-08_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-09_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-10_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-11_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2020-12_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-01_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-02_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 374 rows remaining.\n",
      "After mapping location IDs to coordinates: 374 rows remaining.\n",
      "After filtering by bounding box: 374 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-03_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-04_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-05_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-06_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-07_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-08_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-09_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 382 rows remaining.\n",
      "After mapping location IDs to coordinates: 382 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-10_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-11_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2021-12_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 372 rows remaining.\n",
      "After mapping location IDs to coordinates: 372 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-01_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 382 rows remaining.\n",
      "After mapping location IDs to coordinates: 382 rows remaining.\n",
      "After filtering by bounding box: 382 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-02_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 373 rows remaining.\n",
      "After mapping location IDs to coordinates: 373 rows remaining.\n",
      "After filtering by bounding box: 373 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-03_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-04_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 372 rows remaining.\n",
      "After mapping location IDs to coordinates: 372 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-05_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-06_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-07_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 375 rows remaining.\n",
      "After mapping location IDs to coordinates: 375 rows remaining.\n",
      "After filtering by bounding box: 374 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-08_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-09_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-10_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-11_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 372 rows remaining.\n",
      "After mapping location IDs to coordinates: 372 rows remaining.\n",
      "After filtering by bounding box: 372 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2022-12_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-01_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-02_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-03_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-04_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-05_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-06_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-07_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 375 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-08_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-09_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-10_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-11_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2023-12_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 378 rows remaining.\n",
      "After mapping location IDs to coordinates: 378 rows remaining.\n",
      "After filtering by bounding box: 378 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-01_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-02_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 379 rows remaining.\n",
      "After mapping location IDs to coordinates: 379 rows remaining.\n",
      "After filtering by bounding box: 379 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-03_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-04_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 383 rows remaining.\n",
      "After mapping location IDs to coordinates: 383 rows remaining.\n",
      "After filtering by bounding box: 383 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-05_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 380 rows remaining.\n",
      "After mapping location IDs to coordinates: 380 rows remaining.\n",
      "After filtering by bounding box: 380 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-06_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 376 rows remaining.\n",
      "After mapping location IDs to coordinates: 376 rows remaining.\n",
      "After filtering by bounding box: 376 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-07_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 377 rows remaining.\n",
      "After mapping location IDs to coordinates: 377 rows remaining.\n",
      "After filtering by bounding box: 377 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n",
      "Processing file: C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\\yellow_tripdata_2024-08_sample.parquet\n",
      "Processing DataFrame with 385 rows.\n",
      "After removing invalid location IDs: 381 rows remaining.\n",
      "After mapping location IDs to coordinates: 381 rows remaining.\n",
      "After filtering by bounding box: 381 rows remaining.\n",
      "Processing completed.\n",
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "directory= r\"C:\\Users\\wenji\\OneDrive\\Project\\sampled yellow taxi\"\n",
    "taxi_data=process_and_merge_files(directory, taxi_zones_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "198a78c6-9de2-4330-9de6-e613f69a0721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0   2020-01-25 10:49:58   2020-01-25 11:07:35              1.0           3.28   \n",
      "1   2020-01-15 07:30:08   2020-01-15 07:40:01              1.0           1.75   \n",
      "2   2020-01-09 06:29:09   2020-01-09 06:35:44              1.0           0.87   \n",
      "3   2020-01-26 12:24:04   2020-01-26 12:29:15              2.0           0.98   \n",
      "4   2020-01-30 07:57:53   2020-01-30 08:10:19              1.0           1.30   \n",
      "..                  ...                   ...              ...            ...   \n",
      "95  2020-01-02 18:44:02   2020-01-02 19:04:20              1.0           1.98   \n",
      "96  2020-01-06 15:20:04   2020-01-06 15:25:53              1.0           0.80   \n",
      "97  2020-01-30 14:18:00   2020-01-30 14:58:52              1.0          10.76   \n",
      "98  2020-01-02 08:19:56   2020-01-02 08:30:49              5.0           1.62   \n",
      "99  2020-01-21 21:51:49   2020-01-21 21:53:33              1.0           0.28   \n",
      "\n",
      "    pulocationid  dolocationid  tip_amount  total_amount  \\\n",
      "0          142.0         246.0        1.70         19.00   \n",
      "1          238.0         166.0        1.20         13.00   \n",
      "2          100.0         164.0        0.00          8.80   \n",
      "3          161.0          43.0        0.00          8.80   \n",
      "4          229.0         262.0        2.45         14.75   \n",
      "..           ...           ...         ...           ...   \n",
      "95         162.0         141.0        0.00         17.80   \n",
      "96          79.0         211.0        1.85         11.15   \n",
      "97         234.0         138.0        4.59         50.51   \n",
      "98         107.0         100.0        1.00         13.30   \n",
      "99         141.0         237.0        0.00          7.30   \n",
      "\n",
      "    congestion_surcharge  pickup_latitude  pickup_longitude  dropoff_latitude  \\\n",
      "0                    2.5        40.791705        -73.973049         40.809457   \n",
      "1                    2.5        40.753513        -73.988787         40.748575   \n",
      "2                    2.5        40.758028        -73.977698         40.782478   \n",
      "3                    2.5        40.756729        -73.965146         40.775932   \n",
      "4                    2.5        40.756688        -73.972356         40.758028   \n",
      "..                   ...              ...               ...               ...   \n",
      "95                   2.5        40.727620        -73.985937         40.723888   \n",
      "96                   2.5        40.740337        -73.990458         40.774376   \n",
      "97                   2.5        40.736824        -73.984052         40.753513   \n",
      "98                   2.5        40.766948        -73.959635         40.768615   \n",
      "99                   2.5        40.778766        -73.951010         40.782478   \n",
      "\n",
      "    dropoff_longitude  \n",
      "0          -73.961764  \n",
      "1          -73.985156  \n",
      "2          -73.965554  \n",
      "3          -73.946510  \n",
      "4          -73.977698  \n",
      "..                ...  \n",
      "95         -74.001538  \n",
      "96         -73.873629  \n",
      "97         -73.988787  \n",
      "98         -73.965635  \n",
      "99         -73.965554  \n",
      "\n",
      "[100 rows x 13 columns]\n",
      "Total number of rows: 21182\n"
     ]
    }
   ],
   "source": [
    "print(taxi_data.head(100))\n",
    "total_rows = taxi_data.shape[0]\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea95130a-3640-477f-97ac-a7c1ac47444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the taxi_data DataFrame as a CSV file\n",
    "taxi_data.to_csv(\"taxi_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
